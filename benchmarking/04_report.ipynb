{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reporting ViScore benchmark results\n",
    "\n",
    "Once your benchmark is completed, you can use informative plots to summarise results.\n",
    "We will need a Python environment with ViScore, its dependencies, `funkyheatmappy` and `adjustText`.\n",
    "You can install `funkyheatmappy` and `adjustText` using the following command in your shell:\n",
    "\n",
    "```\n",
    "pip install git+https://github.com/funkyheatmap/funkyheatmappy.git\n",
    "pip install git+https://github.com/Phlya/adjustText.git\n",
    "```\n",
    "\n",
    "We assume that you followed instructions in `README.md` for designing and running your benchmark.\n",
    "In accordance with that, we assume that\n",
    "\n",
    "* results of benchmark are stored in `./results`\n",
    "* all datasets listed in `./datasets.txt` were used\n",
    "* all methods listed in `./config.json` were used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import copy\n",
    "import pandas as pd\n",
    "import funkyheatmappy as fh\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib as mpl\n",
    "import matplotlib.patheffects as pe\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "from adjustText import adjust_text\n",
    "from scipy.spatial import ConvexHull\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "palette = [\n",
    "            '#000000', '#1CE6FF', '#FF34FF', '#FF4A46', '#008941', '#006FA6', '#A30059',\n",
    "            '#7A4900', '#0000A6', '#63FFAC', '#B79762', '#004D43', '#8FB0FF', '#997D87',\n",
    "            '#5A0007', '#809693', '#1B4400', '#4FC601', '#3B5DFF', '#4A3B53', '#FF2F80',\n",
    "            '#61615A', '#BA0900', '#6B7900', '#00C2A0', '#FFAA92', '#FF90C9', '#B903AA', '#D16100',\n",
    "            '#DDEFFF', '#000035', '#7B4F4B', '#A1C299', '#300018', '#0AA6D8', '#013349', '#00846F',\n",
    "            '#372101', '#FFB500', '#C2FFED', '#A079BF', '#CC0744', '#C0B9B2', '#C2FF99', '#001E09',\n",
    "            '#00489C', '#6F0062', '#0CBD66', '#EEC3FF', '#456D75', '#B77B68', '#7A87A1', '#788D66',\n",
    "            '#885578', '#FAD09F', '#FF8A9A', '#D157A0', '#BEC459', '#456648', '#0086ED', '#886F4C',\n",
    "            '#34362D', '#B4A8BD', '#00A6AA', '#452C2C', '#636375', '#A3C8C9', '#FF913F', '#938A81',\n",
    "            '#575329', '#00FECF', '#B05B6F', '#8CD0FF', '#3B9700', '#04F757', '#C8A1A1', '#1E6E00',\n",
    "            '#7900D7', '#A77500', '#6367A9', '#A05837', '#6B002C', '#772600', '#D790FF', '#9B9700',\n",
    "            '#549E79', '#FFF69F', '#201625', '#72418F', '#BC23FF', '#99ADC0', '#3A2465', '#922329',\n",
    "            '#5B4534', '#FDE8DC', '#404E55', '#0089A3', '#CB7E98', '#A4E804', '#324E72', '#6A3A4C'\n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **0.** Collecting results\n",
    "\n",
    "We start by aggregating quantitative results.\n",
    "\n",
    "* The `rnx`, `sl` and `sg` dictionaries will contain denoised and non-denoised RNX, Local SP and Global SP values for all method-dataset combinations.\n",
    "* The `df_all` dataframe will contain all Local SP, Global SP, Balanced SP (using either geometric mean or harmonic mean) and average xNPE values per method-dataset-denoising combination, for each run (random seed).\n",
    "* The `df_agg` dataframe will contain mean and standard-deviation values, aggregating across runs.\n",
    "\n",
    "<hr>\n",
    "\n",
    "The first step is to determine the datasets and methods used in this benchmark.\n",
    "**If this is anything other than what is indicated in `./datasets.txt` and `./config.json`, you will need to adapt this manually.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname_datasets = './datasets.txt'\n",
    "with open(fname_datasets, 'r') as f:\n",
    "  datasets = [line.strip() for line in f.readlines()]\n",
    "fname_config = './config.json'\n",
    "with open(fname_config, encoding='utf-8') as f:\n",
    "    conf = json.load(f)\n",
    "methods = list(conf['methods'].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to specify:\n",
    "\n",
    "* Which target dimensionality we are working with (`zdim`).\n",
    "\n",
    "* How many repeated runs of each set-up we have (`nruns`).\n",
    "\n",
    "* Whether we are working with results for denoised inputs. The `denoised` variable can be `False` (use results on non-denoised data), `True` (use results on denosied data) or `'ViVAE'` (only use denoised data results for ViVAE: this is what ViVAE was designed for).\n",
    "**Crucially, this does not mean ViVAE is evaluated against denoised inputs (this would be an unfair comparison): it is evaluated the same way all other methods are.**\n",
    "\n",
    "* Whether to use `'harmonic_mean'` or `'geometric_mean'` for computing balanced (local-global) structure preservation (there is a case to be made for both)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zdim = 2\n",
    "nruns = 5\n",
    "denoised = 'ViVAE'\n",
    "balanced_measure = 'geometric_mean'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_dicts(datasets, methods, zdim=zdim, nruns=nruns):\n",
    "    def _collect_dict(denoised=False):\n",
    "        rnx = {}\n",
    "        sl = {}\n",
    "        sg = {}\n",
    "        for dataset in datasets:\n",
    "            d_rnx = {}\n",
    "            d_sl = {}\n",
    "            d_sg = {}\n",
    "            for method in methods:\n",
    "                m_rnx = [np.load(f'results/{dataset}_{method}_z{zdim}_u{denoised}/rnx_curve_seed{seed}.npy', allow_pickle=True) for seed in range(1, nruns+1)]        \n",
    "                m_sl = [np.load(f'results/{dataset}_{method}_z{zdim}_u{denoised}/sp_local_seed{seed}.npy', allow_pickle=True) for seed in range(1, nruns+1)]\n",
    "                m_sg = [np.load(f'results/{dataset}_{method}_z{zdim}_u{denoised}/sp_global_seed{seed}.npy', allow_pickle=True) for seed in range(1, nruns+1)]\n",
    "                d_rnx.update({method: m_rnx})\n",
    "                d_sl.update({method: m_sl})\n",
    "                d_sg.update({method: m_sg})\n",
    "            rnx.update({dataset: d_rnx})\n",
    "            sl.update({dataset: d_sl})\n",
    "            sg.update({dataset: d_sg})\n",
    "        return rnx, sl, sg\n",
    "    rnx = {}\n",
    "    sl = {}\n",
    "    sg = {}\n",
    "    rnx_uFalse, sl_uFalse, sg_uFalse = _collect_dict(denoised=False)\n",
    "    rnx_uTrue, sl_uTrue, sg_uTrue = _collect_dict(denoised=True)\n",
    "    rnx.update({'uFalse': rnx_uFalse})\n",
    "    rnx.update({'uTrue':  rnx_uTrue})\n",
    "    sl.update({'uFalse': sl_uFalse})\n",
    "    sl.update({'uTrue':  sl_uTrue})\n",
    "    sg.update({'uFalse': sg_uFalse})\n",
    "    sg.update({'uTrue':  sg_uTrue})\n",
    "    return rnx, sl, sg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_df_avg(\n",
    "    datasets,\n",
    "    methods,\n",
    "    nruns=5,\n",
    "    zdim=zdim,\n",
    "    balanced_measure=balanced_measure, # geometric_mean or harmonic_mean (F-score) to calculate balance between Local and Global SP: debatable which one is more valid\n",
    "    wide=True\n",
    "): # wide or long format\n",
    "    res = []\n",
    "    for dataset in datasets:\n",
    "        for method in methods:\n",
    "            for denoised in [False, True]:\n",
    "                fpath    = os.path.join('results', f'{dataset}_{method}_z{zdim}_u{denoised}')\n",
    "\n",
    "                localsp  = np.array([np.load(os.path.join(fpath, f'sp_local_seed{seed}.npy'), allow_pickle=True) for seed in range(1, nruns+1)])\n",
    "                globalsp = np.array([np.load(os.path.join(fpath, f'sp_global_seed{seed}.npy'), allow_pickle=True) for seed in range(1, nruns+1)])\n",
    "                if balanced_measure=='geometric_mean':\n",
    "                    balsp = np.array([np.sqrt(localsp[i]*globalsp[i]) for i in range(nruns)])\n",
    "                elif balanced_measure=='harmonic_mean':\n",
    "                    balsp = np.array([2*(localsp[i]*globalsp[i])/(localsp[i]+globalsp[i]) for i in range(nruns)])\n",
    "                xnpe     = np.array([np.load(os.path.join(fpath, f'xnpe_seed{seed}.npy'), allow_pickle=True).tolist() for seed in range(1, nruns+1)])\n",
    "                xnpemean = np.array([np.mean(list(x.values())) for x in xnpe])\n",
    "                if wide:\n",
    "                    res.append([\n",
    "                        dataset,\n",
    "                        method,\n",
    "                        zdim,\n",
    "                        denoised,\n",
    "                        np.mean(localsp), np.std(localsp),\n",
    "                        np.mean(globalsp), np.std(globalsp), \n",
    "                        np.mean(balsp), np.std(balsp),\n",
    "                        np.mean(xnpemean), np.std(xnpemean)\n",
    "                    ])\n",
    "                else:\n",
    "                    res.append([dataset, method, zdim, denoised, 'LocalSP', 'Mean', np.mean(localsp)])\n",
    "                    res.append([dataset, method, zdim, denoised, 'LocalSP', 'SD', np.std(localsp)])\n",
    "                    res.append([dataset, method, zdim, denoised, 'GlobalSP', 'Mean', np.mean(globalsp)])\n",
    "                    res.append([dataset, method, zdim, denoised, 'GlobalSP', 'SD', np.std(globalsp)])\n",
    "                    res.append([dataset, method, zdim, denoised, 'BalancedSP', 'Mean', np.mean(balsp)])\n",
    "                    res.append([dataset, method, zdim, denoised, 'BalancedSP', 'SD', np.std(balsp)])\n",
    "                    res.append([dataset, method, zdim, denoised, 'xNPEMean', 'Mean', np.mean(xnpemean)])\n",
    "                    res.append([dataset, method, zdim, denoised, 'xNPEMean', 'SD', np.std(xnpemean)])\n",
    "\n",
    "    if wide:\n",
    "        df_avg = pd.DataFrame(columns=[\n",
    "            'Dataset', 'Method', 'zdim', 'Denoised',\n",
    "            'LocalSP_Mean', 'LocalSP_SD',\n",
    "            'GlobalSP_Mean', 'GlobalSP_SD',\n",
    "            'BalancedSP_Mean', 'BalancedSP_SD',\n",
    "            'xNPEMean_Mean', 'xNPEMean_SD'\n",
    "        ], data = res)\n",
    "    else:\n",
    "        df_avg = pd.DataFrame(columns=[\n",
    "            'Dataset', 'Method', 'zdim', 'Denoised',\n",
    "            'id', 'stat', 'value'\n",
    "        ], data = res)\n",
    "\n",
    "    return df_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_df_all(\n",
    "    datasets,\n",
    "    methods,\n",
    "    nruns=5,\n",
    "    zdim=zdim,\n",
    "    balanced_measure=balanced_measure, # geom_mean or f1 to calculate balance between Local and Global SP: debatable which one is more valid\n",
    "    wide=True\n",
    "): # wide or long format\n",
    "    res = []\n",
    "    for dataset in datasets:\n",
    "        for method in methods:\n",
    "            for denoised in [False, True]:\n",
    "                fpath    = os.path.join('results', f'{dataset}_{method}_z{zdim}_u{denoised}')\n",
    "\n",
    "                localsp  = np.array([np.load(os.path.join(fpath, f'sp_local_seed{seed}.npy'), allow_pickle=True) for seed in range(1, nruns+1)])\n",
    "                globalsp = np.array([np.load(os.path.join(fpath, f'sp_global_seed{seed}.npy'), allow_pickle=True) for seed in range(1, nruns+1)])\n",
    "                if balanced_measure=='geometric_mean':\n",
    "                    balsp = np.array([np.sqrt(localsp[i]*globalsp[i]) for i in range(nruns)])\n",
    "                elif balanced_measure=='harmonic_mean':\n",
    "                    balsp = np.array([2*(localsp[i]*globalsp[i])/(localsp[i]+globalsp[i]) for i in range(nruns)])\n",
    "                xnpe     = np.array([np.load(os.path.join(fpath, f'xnpe_seed{seed}.npy'), allow_pickle=True).tolist() for seed in range(1, nruns+1)])\n",
    "                xnpemean = np.array([np.mean(list(x.values())) for x in xnpe])\n",
    "\n",
    "                if wide:\n",
    "                    for i in range(len(localsp)):\n",
    "                        res.append([dataset, method, zdim, denoised, localsp[i], globalsp[i], balsp[i], xnpemean[i]])\n",
    "                else:\n",
    "                    for i in range(len(localsp)):\n",
    "                        res.append([dataset, method, zdim, denoised, 'LocalSP', localsp[i]])\n",
    "                    for i in range(len(globalsp)):\n",
    "                        res.append([dataset, method, zdim, denoised, 'GlobalSP', globalsp[i]])\n",
    "                    for i in range(len(balsp)):\n",
    "                        res.append([dataset, method, zdim, denoised, 'BalancedSP', balsp[i]])\n",
    "                    for i in range(len(xnpemean)):\n",
    "                        res.append([dataset, method, zdim, denoised, 'xNPEMean', xnpemean[i]])\n",
    "\n",
    "    if wide:\n",
    "        df_all = pd.DataFrame(columns=[\n",
    "            'Dataset', 'Method', 'zdim', 'Denoised', 'LocalSP', 'GlobalSP', 'BalancedSP', 'xNPEMean'\n",
    "        ], data = res)\n",
    "    else:\n",
    "        df_all = pd.DataFrame(columns=[\n",
    "            'Dataset', 'Method', 'zdim', 'Denoised', 'id', 'value'\n",
    "        ], data = res)\n",
    "\n",
    "    return df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_denoised_mask(df, denoised):\n",
    "    if denoised==True or denoised==False:\n",
    "        mask_denoised = np.array(df['Denoised']==denoised)\n",
    "    elif denoised=='ViVAE':\n",
    "        mask_denoised = np.logical_or.reduce([\n",
    "            np.logical_and.reduce([df['Denoised']==True, pd.Series([x.startswith('ViVAE') for x in df['Method']])]),\n",
    "            np.logical_and.reduce([df['Denoised']==False, pd.Series([not x.startswith('ViVAE') for x in df['Method']])])\n",
    "        ])\n",
    "    return mask_denoised"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's collect the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnx, sl, sg = collect_dicts(datasets, methods)\n",
    "df_avg = collect_df_avg(datasets, methods, wide=True)\n",
    "df_all = collect_df_all(datasets, methods, wide=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1.** Reporting structure-preservation values\n",
    "\n",
    "We will plot the Local, Global and Balanced SP using scatterplots with errorbars for separate categories and a scatterplot showing the Local-Global trade-off.\n",
    "\n",
    "<hr>\n",
    "\n",
    "First, the separate plotting of Local, Global and Balanced SP, using points with error bars (mean and standard deviation), separately also for each dataset.\n",
    "This is not the easiest plot to look at, but we make it for the sake of completeness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_separate_sp(datasets, methods, df_all, denoised='ViVAE'):\n",
    "    mpl.rcParams['axes.linewidth'] = 0.1\n",
    "    fname = ['./plots/01_sp_separate.png', './plots/01_sp_separate.svg']\n",
    "    fig, ax = plt.subplots(nrows=len(datasets), ncols=3, figsize=(4.5, .9*len(datasets)), sharey=True, dpi=150)\n",
    "    fig.subplots_adjust(hspace=.5)\n",
    "    for i, dataset in enumerate(datasets):\n",
    "\n",
    "        for j, method in enumerate(methods):\n",
    "\n",
    "            mask_dataset = df_all['Dataset']==dataset\n",
    "            mask_method = df_all['Method']==method\n",
    "            mask_denoised = get_denoised_mask(df_all, denoised=denoised)\n",
    "            mask = np.logical_and.reduce([mask_dataset, mask_method, mask_denoised])\n",
    "\n",
    "            ## Local SP\n",
    "            d = df_all['LocalSP'][mask]\n",
    "            ax[i][0].errorbar(x=np.mean(d), y=j, xerr=np.std(d), label=method, color=palette[j], markersize=1.8, alpha=.9, fmt='o', linewidth=1.6)\n",
    "            ax[i][0].grid(visible=True, axis='x', ls='--')\n",
    "\n",
    "            ## Global SP\n",
    "            d = df_all['GlobalSP'][mask]\n",
    "            ax[i][1].errorbar(x=np.mean(d), y=j, xerr=np.std(d), label=method, color=palette[j], markersize=1.8, alpha=.9, fmt='o', linewidth=1.6)\n",
    "            ax[i][1].grid(visible=True, axis='x', ls='--')\n",
    "\n",
    "            ## Balanced SP\n",
    "            d = df_all['BalancedSP'][mask]\n",
    "            ax[i][2].errorbar(x=np.mean(d), y=j, xerr=np.std(d), label=method, color=palette[j], markersize=1.8, alpha=.9, fmt='o', linewidth=1.6)\n",
    "            ax[i][2].grid(visible=True, axis='x', ls='--')\n",
    "\n",
    "        ax[i][0].set_yticks(ticks=range(len(methods)), labels=methods)\n",
    "        ax[i][1].yaxis.set_tick_params(left=False)\n",
    "        ax[i][2].yaxis.set_tick_params(left=False)\n",
    "        for j in [0, 1, 2]:\n",
    "            ax[i][j].tick_params(axis='both', labelsize=5)\n",
    "    pad = 5\n",
    "    for a, row in zip(ax[:,0], datasets):\n",
    "            a.annotate(row, xy=(0, .5), xytext=(0, 0), xycoords=a.yaxis.label, textcoords='offset points', size=6, weight='bold', ha='right', va='center')\n",
    "    for a, col in zip(ax[0], ['Local SP', 'Global SP', 'Balanced SP']):\n",
    "            a.annotate(col, xy=(.5, 1), xytext=(0, pad), xycoords='axes fraction', textcoords='offset points', size=6, weight='bold', ha='center', va='baseline')\n",
    "    fig.suptitle('A', x=-.15, y=.9, size=12, weight='bold')\n",
    "    if fname is not None:\n",
    "        if isinstance(fname, list) and len(fname)>0:\n",
    "            for f in fname:\n",
    "                if f.endswith('.png'):\n",
    "                    fig.savefig(f, bbox_inches='tight', dpi=300, transparent=True)\n",
    "                else:\n",
    "                    fig.savefig(f, bbox_inches='tight', transparent=True)\n",
    "        elif isinstance(fname, str):\n",
    "                fig.savefig(fname, bbox_inches='tight', transparent=True)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_separate_sp(datasets, methods, df_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, we plot the trade-off/balance between Local and Global by the means of a scatter plot.\n",
    "We do this dataset by dataset, using the *x*-axis for Local SP, the *y*-axis for Global SP.\n",
    "Each method/set-up gets a point place where the mean values are for each measure, and then an ellipse centered around the mean gets drawn to indicate standard deviation for both measures.\n",
    "The ellipses can turn out barely visible or invisible in case the standard deviation is small relative to the axis range.\n",
    "We use text labels for each point & ellipse, to indicate which method is which.\n",
    "\n",
    "Additionally, we make one plot where we aggregate results across all datasets, to indicate which method falls into which quadrant of the plot generally (low-low, low-high, high-low or high-high with respect to Local and Global SP).\n",
    "\n",
    "If `pareto` is set to True, we put an asterisk next to the name of each method that lies on the Pareto front of the results for a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sp_tradeoffs(datasets, methods, df_avg, pareto=True, denoised='ViVAE'):\n",
    "    for joint in [False, True]:\n",
    "        fpath_plots = './plots/03_sp_tradeoffs/'\n",
    "        if not os.path.exists(fpath_plots):\n",
    "            os.mkdir(fpath_plots)\n",
    "\n",
    "        if joint:\n",
    "            fig, ax = plt.subplots(figsize=(1.8,1.8), dpi=150)\n",
    "        for i, dataset in enumerate(datasets):\n",
    "            if not joint:\n",
    "                fig, ax = plt.subplots(figsize=(1.8,1.8), dpi=150)\n",
    "            xlims = np.array([np.min(df_all['LocalSP'])-.05, np.max(df_all['LocalSP'])+.05])\n",
    "            ylims = np.array([np.min(df_all['GlobalSP'])-.05, np.max(df_all['GlobalSP'])+.05])\n",
    "            \n",
    "            mask_dataset = df_avg['Dataset']==dataset\n",
    "            xcoords = []\n",
    "            ycoords = []\n",
    "            ax.set_xlim(xlims)\n",
    "            ax.set_ylim(ylims)\n",
    "            for j, method in enumerate(methods):\n",
    "                mask_method = df_avg['Method']==method\n",
    "                if denoised==True or denoised==False:\n",
    "                    mask_denoised = df_avg['Denoised']==denoised\n",
    "                elif denoised=='ViVAE':\n",
    "                    mask_denoised = df_avg['Denoised']==True if method.startswith('ViVAE') else df_avg['Denoised']==False\n",
    "                mask = np.logical_and.reduce([mask_dataset, mask_method, mask_denoised])\n",
    "\n",
    "                mu_local     = df_avg['LocalSP_Mean'][mask]\n",
    "                mu_global    = df_avg['GlobalSP_Mean'][mask]\n",
    "                sigma_local  = df_avg['LocalSP_SD'][mask]\n",
    "                sigma_global = df_avg['GlobalSP_SD'][mask]\n",
    "\n",
    "                xcoords.append(mu_local)    \n",
    "                ycoords.append(mu_global)\n",
    "\n",
    "                if not joint:\n",
    "                    ellipse = mpl.patches.Ellipse(xy=(mu_local, mu_global), width=sigma_local, height=sigma_global, color=palette[j], alpha=.3)\n",
    "\n",
    "                    ax.add_patch(ellipse)\n",
    "                \n",
    "                    ax.vlines(x=mu_local, ymin=np.repeat(0., len(mu_global)), ymax=mu_global, color='gray', lw=.5, linestyles='dashed', zorder=1)\n",
    "                    ax.hlines(y=mu_global, xmin=np.repeat(0., len(mu_local)), xmax=mu_local, color='gray', lw=.5, linestyles='dashed', zorder=1)\n",
    "                \n",
    "                ax.scatter(x=mu_local, y=mu_global, s=25 if joint else 8, marker='.', color=palette[j], label=method, alpha=.8, zorder=2)\n",
    "\n",
    "                ax.set_xlabel('Local SP', fontsize=5)\n",
    "                ax.set_ylabel('Global SP', fontsize=5)\n",
    "\n",
    "            if not joint:\n",
    "\n",
    "                method_labs = copy.deepcopy(methods)\n",
    "                if pareto:\n",
    "                    scores = np.hstack([np.array(xcoords), np.array(ycoords)])\n",
    "                    costs = 1-scores\n",
    "                    eff = np.ones(costs.shape[0], dtype=bool)\n",
    "                    for i, c in enumerate(costs):\n",
    "                        eff[i] = np.all(np.any(costs[:i] > c, axis=1)) and np.all(np.any(costs[i+1:] > c, axis=1))\n",
    "                    pareto_idcs = np.where(eff)[0]\n",
    "                    for k in pareto_idcs:\n",
    "                        method_labs[k] = f'{method_labs[k]}*'\n",
    "\n",
    "                texts = []\n",
    "                for k, (x, y, s) in enumerate(zip(xcoords, ycoords, method_labs)):\n",
    "                    texts.append(plt.text(x, y, s, color=palette[k], size=5, path_effects=[pe.withStroke(linewidth=.2, foreground='black')]))\n",
    "                adjust_text(texts, force_points=10., force_text=30., arrowprops=dict(arrowstyle='-', color='lightgray', alpha=.5, lw=.8))\n",
    "            \n",
    "            if not joint:\n",
    "                ax.annotate(dataset, xy=(-.8, .5), xytext=(0, 0), xycoords=ax.yaxis.label, textcoords='offset points', size=6, weight='bold', ha='right', va='center')\n",
    "                ax.tick_params(axis='both', labelsize=5)\n",
    "                fig.savefig(f'plots/03_sp_tradeoffs/03_sp_tradeoff_{dataset}.svg', bbox_inches='tight', transparent=True)\n",
    "                fig.savefig(f'plots/03_sp_tradeoffs/03_sp_tradeoff_{dataset}.png', bbox_inches='tight', dpi=300, transparent=True)\n",
    "\n",
    "        if joint:\n",
    "            ax.annotate('All datasets', xy=(-.8, .5), xytext=(0, 0), xycoords=ax.yaxis.label, textcoords='offset points', size=6, weight='bold', ha='right', va='center')\n",
    "            ax.tick_params(axis='both', labelsize=5)\n",
    "\n",
    "            fig.savefig('plots/03_sp_tradeoffs/03_sp_tradeoff_JOINT.svg', bbox_inches='tight', transparent=True)\n",
    "            fig.savefig('plots/03_sp_tradeoffs/03_sp_tradeoff_JOINT.png', bbox_inches='tight', dpi=300, transparent=True)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "plot_sp_tradeoffs(datasets, methods, df_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.** Reporting quantitative results in a heatmap\n",
    "\n",
    "We use a [funky heatmap](https://funkyheatmap.github.io/funkyheatmap/) to plot Local, Global and Balanced SP.\n",
    "This is a popular visualisation method used often in benchmarking reports.\n",
    "In order for things to work correctly for us, we need to monkey-patch some functions in the `funkyheatmappy` module.\n",
    "\n",
    "**The plotted values are min-max scaled for each of the 3 categories.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Monkey-patch (long)\n",
    "\n",
    "def calculate_positions(\n",
    "    data,\n",
    "    column_info,\n",
    "    row_info,\n",
    "    column_groups,\n",
    "    row_groups,\n",
    "    palettes,\n",
    "    position_args,\n",
    "    scale_column,\n",
    "    add_abc,\n",
    "):\n",
    "    row_height = position_args[\"row_height\"]\n",
    "    row_space = position_args[\"row_space\"]\n",
    "    row_bigspace = position_args[\"row_bigspace\"]\n",
    "    col_width = position_args[\"col_width\"]\n",
    "    col_space = position_args[\"col_space\"]\n",
    "    col_bigspace = position_args[\"col_bigspace\"]\n",
    "    col_annot_offset = position_args[\"col_annot_offset\"]\n",
    "    col_annot_angle = position_args[\"col_annot_angle\"]\n",
    "\n",
    "    # Determine row positions\n",
    "    if not \"group\" in row_info.columns or all(pd.isna(row_info[\"group\"])):\n",
    "        row_info[\"group\"] = \"\"\n",
    "        row_groups = pd.DataFrame({\"group\": [\"\"]})\n",
    "        plot_row_annotation = False\n",
    "    else:\n",
    "        plot_row_annotation = True\n",
    "\n",
    "    row_pos = fh.calculate_row_positions.calculate_row_positions(\n",
    "        row_info=row_info,\n",
    "        row_height=row_height,\n",
    "        row_space=row_space,\n",
    "        row_bigspace=row_bigspace,\n",
    "    )\n",
    "\n",
    "    # Determine column positions\n",
    "    if not \"group\" in column_info.columns or all(pd.isna(column_info[\"group\"])):\n",
    "        column_info[\"group\"] = \"\"\n",
    "        column_groups = pd.DataFrame({\"group\": [\"\"]})\n",
    "        plot_column_annotation = False\n",
    "    else:\n",
    "        plot_column_annotation = True\n",
    "\n",
    "    column_pos = fh.calculate_column_positions.calculate_column_positions(\n",
    "        column_info=column_info, col_space=col_space, col_bigspace=col_bigspace\n",
    "    )\n",
    "\n",
    "    # Process data\n",
    "    data_processor = fh.make_data_processor.make_data_processor(\n",
    "        data=data,\n",
    "        column_pos=column_pos,\n",
    "        row_pos=row_pos,\n",
    "        scale_column=scale_column,\n",
    "        palette_list=palettes,\n",
    "    )\n",
    "\n",
    "    def circle_fun(dat):\n",
    "        dat = dat.assign(x0=dat[\"x\"], y0=dat[\"y\"], r=row_height / 2 * dat[\"value\"])\n",
    "        return dat\n",
    "\n",
    "    circle_data = data_processor(\"circle\", circle_fun)\n",
    "\n",
    "    def rect_fun(dat):\n",
    "        return dat\n",
    "\n",
    "    rect_data = data_processor(\"rect\", rect_fun)\n",
    "\n",
    "    def funkyrect_fun(dat):\n",
    "        result = pd.concat(\n",
    "            [\n",
    "                fh.score_to_funkyrectangle.score_to_funkyrectangle(\n",
    "                    xmin=row[\"xmin\"],\n",
    "                    xmax=row[\"xmax\"],\n",
    "                    ymin=row[\"ymin\"],\n",
    "                    ymax=row[\"ymax\"],\n",
    "                    value=row[\"value\"],\n",
    "                    midpoint=0.8,\n",
    "                )\n",
    "                for _, row in dat[[\"xmin\", \"xmax\", \"ymin\", \"ymax\", \"value\"]].iterrows()\n",
    "            ]\n",
    "        )\n",
    "        return result\n",
    "\n",
    "    funkyrect_data = data_processor(\"funkyrect\", funkyrect_fun)\n",
    "\n",
    "    def bar_fun(dat):\n",
    "        dat = fh.add_column_if_missing.add_column_if_missing(dat, hjust=0)\n",
    "        dat = dat.assign(\n",
    "            xmin=dat[\"xmin\"] + (1 - dat[\"value\"]) * dat[\"xwidth\"] * dat[\"hjust\"],\n",
    "            xmax=dat[\"xmax\"] - (1 - dat[\"value\"]) * dat[\"xwidth\"] * (1 - dat[\"hjust\"]),\n",
    "        )\n",
    "        return dat\n",
    "\n",
    "    bar_data = data_processor(\"bar\", bar_fun)\n",
    "\n",
    "    def barguides_fun(dat):\n",
    "        dat = ((dat.groupby(\"column_id\").first())[[\"xmin\", \"xmax\"]]).melt(\n",
    "            var_name=\"col\", value_name=\"x\"\n",
    "        )\n",
    "        dat = dat.assign(xend=dat[\"x\"])[[\"x\", \"xend\"]]\n",
    "        cols_to_add = pd.DataFrame({\"y\": row_pos[\"ymin\"], \"yend\": row_pos[\"ymax\"]})\n",
    "        result = (\n",
    "            pd.merge(dat.assign(key=1), cols_to_add.assign(key=1), on=\"key\")\n",
    "            .drop(\"key\", axis=1)\n",
    "            .sort_values([\"x\", \"xend\"])\n",
    "            .reset_index(drop=True)\n",
    "            .drop_duplicates()\n",
    "            .assign(palette=np.nan, value=np.nan)\n",
    "        )\n",
    "        return result\n",
    "\n",
    "    segment_data = data_processor(\"bar\", barguides_fun).assign(\n",
    "        colour=\"black\", size=0.5, linestyle=\"dashed\"\n",
    "    )\n",
    "\n",
    "    def text_fun(dat):\n",
    "        dat = dat.assign(color=\"black\")\n",
    "        return dat\n",
    "\n",
    "    text_data = data_processor(\"text\", text_fun)\n",
    "\n",
    "    def pie_fun(dat):\n",
    "        result = pd.DataFrame()\n",
    "        for _, row in dat.iterrows():\n",
    "            value_df = pd.DataFrame(row[\"value\"], index=[\"end_angle\"]).transpose()\n",
    "            pctgs = value_df[\"end_angle\"] / value_df[\"end_angle\"].sum()\n",
    "            value_df = (value_df / value_df.sum()) * 360\n",
    "            value_df = value_df.cumsum().fillna(0)\n",
    "            value_df[\"start_angle\"] = value_df[\"end_angle\"].shift(1).fillna(0)\n",
    "            value_df = value_df.loc[value_df[\"start_angle\"] != value_df[\"end_angle\"], :]\n",
    "\n",
    "            end_angle = (-1 * value_df[\"start_angle\"] + 90) % 360\n",
    "            start_angle = (-1 * value_df[\"end_angle\"] + 90) % 360\n",
    "            value_df[\"start_angle\"], value_df[\"end_angle\"] = start_angle, end_angle\n",
    "\n",
    "            value_df[\"height\"] = row_height / 2\n",
    "            value_df[\"x0\"] = row[\"x\"]\n",
    "            value_df[\"y0\"] = row[\"y\"]\n",
    "            value_df[\"row_id\"] = row[\"row_id\"]\n",
    "            value_df[\"value\"] = value_df.index\n",
    "            value_df[\"pctgs\"] = pctgs\n",
    "            result = pd.concat([result, value_df])\n",
    "        result = result.dropna(subset=\"value\", axis=0)\n",
    "        dat = result.merge(dat.drop(\"value\", axis=1), on=[\"row_id\"], how=\"left\")\n",
    "        return dat\n",
    "\n",
    "    pie_data = data_processor(\"pie\", pie_fun)\n",
    "\n",
    "    def image_fun(dat):\n",
    "        dat = dat.assign(y0=dat[\"y\"] - row_height, height=row_height, width=row_height)\n",
    "        return dat\n",
    "\n",
    "    image_data = data_processor(\"image\", image_fun)\n",
    "\n",
    "    # Add Annotations\n",
    "    if plot_row_annotation:\n",
    "        row_annotation = row_groups.melt(\n",
    "            id_vars=\"group\", var_name=\"level\", value_name=\"name\"\n",
    "        ).merge(row_pos[[\"group\", \"ymin\", \"ymax\"]], how=\"left\", on=\"group\")\n",
    "\n",
    "        row_annotation = pd.DataFrame(\n",
    "            {\n",
    "                \"ymin\": row_annotation.groupby(\"name\").apply(lambda x: min(x[\"ymin\"])),\n",
    "                \"ymax\": row_annotation.groupby(\"name\").apply(lambda x: max(x[\"ymax\"])),\n",
    "            }\n",
    "        )\n",
    "        row_annotation[\"y\"] = (row_annotation[\"ymin\"] + row_annotation[\"ymax\"]) / 2\n",
    "        row_annotation[\"xmin\"] = -0.5\n",
    "        row_annotation[\"xmax\"] = 5\n",
    "        row_annotation = row_annotation[\n",
    "            (~pd.isna(row_annotation.index)) & (row_annotation.index != \"\")\n",
    "        ]\n",
    "\n",
    "        text_data_rows = pd.DataFrame(\n",
    "            {\n",
    "                \"xmin\": row_annotation[\"xmin\"],\n",
    "                \"xmax\": row_annotation[\"xmax\"],\n",
    "                \"ymin\": row_annotation[\"ymax\"] + row_space,\n",
    "                \"label_value\": [re.sub(\"\\n\", \" \", x) for x in row_annotation.index],\n",
    "                \"ha\": 0,\n",
    "                \"va\": 0.5,\n",
    "                \"fontweight\": \"bold\",\n",
    "                \"ymax\": (row_annotation[\"ymax\"] + row_space) + row_height,\n",
    "            }\n",
    "        )\n",
    "        text_data = pd.concat([text_data, text_data_rows])\n",
    "\n",
    "    if plot_column_annotation:\n",
    "        col_join = column_groups.melt(\n",
    "            id_vars=[\"group\", \"palette\"], var_name=\"level\", value_name=\"name\"\n",
    "        ).merge(column_pos[[\"group\", \"xmin\", \"xmax\"]], how=\"left\", on=\"group\")\n",
    "        text_pct = 0.9\n",
    "        level_heights = pd.DataFrame(\n",
    "            col_join.groupby(\"level\").apply(lambda x: max(x[\"name\"].str.count(\"\\n\"))),\n",
    "            columns=[\"max_newlines\"],\n",
    "        )\n",
    "        level_heights[\"height\"] = (level_heights[\"max_newlines\"] + 1) * text_pct + (\n",
    "            1 - text_pct\n",
    "        )\n",
    "        level_heights[\"levelmatch\"] = pd.Series(\n",
    "            [column_groups.columns.tolist().index(x) for x in level_heights.index],\n",
    "            index=level_heights.index,\n",
    "            name=\"level\",\n",
    "        )\n",
    "        level_heights = level_heights.sort_values([\"levelmatch\"], ascending=False)\n",
    "        level_heights[\"ysep\"] = row_space\n",
    "        level_heights[\"ymax\"] = (\n",
    "            col_annot_offset\n",
    "            + (level_heights[\"height\"] + level_heights[\"ysep\"]).cumsum()\n",
    "            - level_heights[\"ysep\"]\n",
    "        )\n",
    "        level_heights[\"ymin\"] = level_heights[\"ymax\"] - level_heights[\"height\"]\n",
    "        level_heights[\"y\"] = (level_heights[\"ymin\"] + level_heights[\"ymax\"]) / 2\n",
    "        palette_mids = {\n",
    "            x: palettes[x][round(len(palettes[x]) / 2)]\n",
    "            if isinstance(palettes[x], list)\n",
    "            else list(palettes[x].values())[round(len(palettes[x]) / 2)]\n",
    "            for x in palettes.keys()\n",
    "        }\n",
    "        max_newlines = (\n",
    "            col_join.groupby(\"level\")\n",
    "            .apply(lambda x: x[\"name\"].str.count(\"\\n\").max())\n",
    "            .transpose()\n",
    "        )\n",
    "        column_annotation = col_join.merge(\n",
    "            max_newlines.rename(\"max_newlines\"), on=\"level\", how=\"left\"\n",
    "        )\n",
    "        xmin = column_annotation.groupby(\n",
    "            [\"level\", \"name\", \"palette\"], dropna=False\n",
    "        ).apply(lambda x: min(x[\"xmin\"]))\n",
    "        xmax = column_annotation.groupby(\n",
    "            [\"level\", \"name\", \"palette\"], dropna=False\n",
    "        ).apply(lambda x: max(x[\"xmax\"]))\n",
    "        column_annotation = (\n",
    "            pd.concat(\n",
    "                [\n",
    "                    xmin.index.to_frame(),\n",
    "                    xmin.rename(\"xmin\"),\n",
    "                    xmax.rename(\"xmax\"),\n",
    "                    ((xmin + xmax) / 2).rename(\"x\"),\n",
    "                ],\n",
    "                axis=1,\n",
    "            )\n",
    "        ).reset_index(drop=True)\n",
    "\n",
    "        column_annotation = column_annotation.merge(\n",
    "            level_heights, on=\"level\", how=\"left\"\n",
    "        )\n",
    "        column_annotation = column_annotation[~pd.isna(column_annotation[\"name\"])]\n",
    "        column_annotation = column_annotation[\n",
    "            column_annotation[\"name\"].str.contains(\"[a-zA-Z]\")\n",
    "        ]\n",
    "        column_annotation[\"colour\"] = [\n",
    "            palette_mids[col] for col in column_annotation[\"palette\"]\n",
    "        ]\n",
    "        rect_data = pd.concat(\n",
    "            [\n",
    "                rect_data,\n",
    "                pd.DataFrame(\n",
    "                    {\n",
    "                        \"xmin\": column_annotation[\"xmin\"],\n",
    "                        \"xmax\": column_annotation[\"xmax\"],\n",
    "                        \"ymin\": column_annotation[\"ymin\"],\n",
    "                        \"ymax\": column_annotation[\"ymax\"],\n",
    "                        \"colour\": column_annotation[\"colour\"],\n",
    "                        \"alpha\": [\n",
    "                            1 if lm == 0 else 0.25\n",
    "                            for lm in column_annotation[\"levelmatch\"]\n",
    "                        ],\n",
    "                        \"border\": False,\n",
    "                    }\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "        text_data = pd.concat(\n",
    "            [\n",
    "                text_data,\n",
    "                pd.DataFrame(\n",
    "                    {\n",
    "                        \"xmin\": column_annotation[\"xmin\"] + col_space,\n",
    "                        \"xmax\": column_annotation[\"xmax\"] - col_space,\n",
    "                        \"ymin\": column_annotation[\"ymin\"],\n",
    "                        \"ymax\": column_annotation[\"ymax\"],\n",
    "                        \"va\": 0.5,\n",
    "                        \"ha\": 0.5,\n",
    "                        \"fontweight\": [\n",
    "                            \"bold\" if lm == 0 else np.nan\n",
    "                            for lm in column_annotation[\"levelmatch\"]\n",
    "                        ],\n",
    "                        \"colour\": [\n",
    "                            \"white\" if lm == 0 else \"black\"\n",
    "                            for lm in column_annotation[\"levelmatch\"]\n",
    "                        ],\n",
    "                        \"label_value\": column_annotation[\"name\"],\n",
    "                    }\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        if add_abc:\n",
    "            alphabet = list(map(chr, range(97, 123)))\n",
    "            c_a_df = (\n",
    "                column_annotation[column_annotation[\"levelmatch\"] == 0]\n",
    "                .sort_values(\"x\")\n",
    "                .reset_index(drop=True)\n",
    "            )\n",
    "            text_data_abc = pd.DataFrame(\n",
    "                {\n",
    "                    \"xmin\": c_a_df[\"xmin\"] + col_space,\n",
    "                    \"xmax\": c_a_df[\"xmax\"] - col_space,\n",
    "                    \"ymin\": c_a_df[\"ymin\"],\n",
    "                    \"ymax\": c_a_df[\"ymax\"],\n",
    "                    \"va\": 0.5,\n",
    "                    \"ha\": 0,\n",
    "                    \"fontweight\": \"bold\",\n",
    "                    \"colour\": \"white\",\n",
    "                    \"label_value\": [alphabet[i] + \")\" for i in c_a_df.index],\n",
    "                }\n",
    "            )\n",
    "            text_data = pd.concat([text_data, text_data_abc])\n",
    "\n",
    "    # Add column names\n",
    "    df = column_pos[column_pos[\"name\"] != \"\"]\n",
    "    if df.shape[0] > 0:\n",
    "        df_column_segments = pd.DataFrame(\n",
    "            {\"x\": df[\"x\"], \"xend\": df[\"x\"], \"y\": -0.3, \"yend\": -0.1, \"size\": 0.5}\n",
    "        )\n",
    "        segment_data = pd.concat([segment_data, df_column_segments])\n",
    "        df_column_text = pd.DataFrame(\n",
    "            {\n",
    "                \"xmin\": df[\"xmin\"],\n",
    "                \"xmax\": df[\"xmax\"],\n",
    "                \"ymin\": 0,\n",
    "                \"ymax\": col_annot_offset,\n",
    "                \"angle\": col_annot_angle,\n",
    "                \"va\": 0,\n",
    "                \"ha\": 0,\n",
    "                \"label_value\": df[\"name\"],\n",
    "            }\n",
    "        )\n",
    "        text_data = pd.concat([text_data, df_column_text])\n",
    "\n",
    "    # Determine plotting window\n",
    "    minimum_x = min(\n",
    "        [\n",
    "            min(lst, default=np.nan)\n",
    "            for lst in [\n",
    "                column_pos[\"xmin\"],\n",
    "                segment_data[\"x\"],\n",
    "                segment_data[\"xend\"],\n",
    "                rect_data[\"xmin\"],\n",
    "                circle_data[\"x\"] - circle_data[\"r\"],\n",
    "                funkyrect_data[\"x\"] - funkyrect_data[\"r\"],\n",
    "                pie_data[\"xmin\"],\n",
    "                text_data[\"xmin\"],\n",
    "            ]\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    maximum_x = max(\n",
    "        [\n",
    "            max(lst, default=np.nan)\n",
    "            for lst in [\n",
    "                column_pos[\"xmax\"],\n",
    "                segment_data[\"x\"],\n",
    "                segment_data[\"xend\"],\n",
    "                rect_data[\"xmax\"],\n",
    "                circle_data[\"x\"] + circle_data[\"r\"],\n",
    "                funkyrect_data[\"x\"] + funkyrect_data[\"r\"],\n",
    "                pie_data[\"xmax\"],\n",
    "                text_data[\"xmax\"],\n",
    "            ]\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    minimum_y = min(\n",
    "        [\n",
    "            min(lst, default=np.nan)\n",
    "            for lst in [\n",
    "                row_pos[\"ymin\"],\n",
    "                segment_data[\"y\"],\n",
    "                segment_data[\"yend\"],\n",
    "                rect_data[\"ymin\"],\n",
    "                circle_data[\"y\"] - circle_data[\"r\"],\n",
    "                funkyrect_data[\"y\"] - funkyrect_data[\"r\"],\n",
    "                pie_data[\"ymin\"],\n",
    "                text_data[\"ymin\"],\n",
    "            ]\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    maximum_y = max(\n",
    "        [\n",
    "            max(lst, default=np.nan)\n",
    "            for lst in [\n",
    "                row_pos[\"ymax\"],\n",
    "                segment_data[\"y\"],\n",
    "                segment_data[\"yend\"],\n",
    "                rect_data[\"ymax\"],\n",
    "                circle_data[\"y\"] + circle_data[\"r\"],\n",
    "                funkyrect_data[\"y\"] + funkyrect_data[\"r\"],\n",
    "                pie_data[\"ymax\"],\n",
    "                text_data[\"ymax\"],\n",
    "            ]\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    # Simplify certain geoms\n",
    "    if funkyrect_data.shape[0] > 0:\n",
    "        circle_data = pd.concat(\n",
    "            [\n",
    "                circle_data,\n",
    "                funkyrect_data[\n",
    "                    ~np.isnan(funkyrect_data[\"start\"])\n",
    "                    & (funkyrect_data[\"start\"] < 1e-10)\n",
    "                    & (2 * np.pi - 1e-10 < funkyrect_data[\"end\"])\n",
    "                ][[\"x\", \"y\", \"r\", \"colour\"]],\n",
    "            ]\n",
    "        )\n",
    "        funkyrect_data = funkyrect_data[\n",
    "            ~(\n",
    "                ~np.isnan(funkyrect_data[\"start\"])\n",
    "                & (funkyrect_data[\"start\"] < 1e-10)\n",
    "                & (2 * np.pi - 1e-10 < funkyrect_data[\"end\"])\n",
    "            )\n",
    "        ]\n",
    "\n",
    "    rect_data = pd.concat([rect_data, bar_data])\n",
    "    return {\n",
    "        \"row_pos\": row_pos,\n",
    "        \"column_pos\": column_pos,\n",
    "        \"segment_data\": segment_data,\n",
    "        \"rect_data\": rect_data,\n",
    "        \"circle_data\": circle_data,\n",
    "        \"funkyrect_data\": funkyrect_data,\n",
    "        \"pie_data\": pie_data,\n",
    "        \"text_data\": text_data,\n",
    "        \"image_data\": image_data,\n",
    "        \"bounds\": {\n",
    "            \"minimum_x\": minimum_x,\n",
    "            \"maximum_x\": maximum_x,\n",
    "            \"minimum_y\": minimum_y,\n",
    "            \"maximum_y\": maximum_y,\n",
    "        },\n",
    "        \"viz_params\": row_space,\n",
    "    }\n",
    "\n",
    "fh.funkyheatmappy.calculate_positions = calculate_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_funky_heatmap(df, datasets, methods):\n",
    "    plt.figure(figsize=(1.5, 6), dpi=150)\n",
    "\n",
    "    def rescale_to_range(vals, lims):\n",
    "        return vals-np.min(vals)/(np.max(vals)-np.min(vals))*(lims[1]-lims[0])+lims[0]\n",
    "\n",
    "    ## Re-scale values for plotting\n",
    "\n",
    "    mask_denoised = get_denoised_mask(df=df, denoised=denoised)\n",
    "\n",
    "    dd = copy.deepcopy(df)\n",
    "    dd = dd[mask_denoised]\n",
    "    dd = dd[['Dataset', 'Method', 'LocalSP_Mean', 'GlobalSP_Mean', 'BalancedSP_Mean']]\n",
    "    dd = dd.rename(columns={'LocalSP_Mean': 'LocalSP', 'GlobalSP_Mean': 'GlobalSP', 'BalancedSP_Mean': 'BalancedSP'})\n",
    "    dd[['LocalSP', 'GlobalSP', 'BalancedSP']] = MinMaxScaler().fit_transform(dd[['LocalSP', 'GlobalSP', 'BalancedSP']])\n",
    "\n",
    "    ## Gather data for funky heatmap\n",
    "\n",
    "    dhf = pd.DataFrame.from_dict({'id': methods})\n",
    "    dhf.index = methods\n",
    "\n",
    "    col_groups = [pd.NA]\n",
    "    col_names = ['']\n",
    "    col_geoms = ['text']\n",
    "    col_pals = [np.nan]\n",
    "    col_opts = [{'ha': 1., 'width':3.}]\n",
    "    col_legs = [False]\n",
    "\n",
    "    for i, dataset in enumerate(datasets):\n",
    "        mask = dd['Dataset']==dataset\n",
    "        d = dd[mask]\n",
    "        d = d[['LocalSP', 'GlobalSP', 'BalancedSP']]\n",
    "        d = d.rename(columns={'LocalSP': f'LocalSP_{dataset}', 'GlobalSP': f'GlobalSP_{dataset}', 'BalancedSP': f'BalancedSP_{dataset}'})\n",
    "        col_groups.extend([dataset, dataset, dataset])\n",
    "        col_names.extend(['Local SP', 'Global SP', 'Balanced SP'])\n",
    "        col_geoms.extend(['funkyrect', 'funkyrect', 'funkyrect'])\n",
    "        palette_to_use = f'palette{np.mod(i,2)+1}'\n",
    "        col_pals.extend([palette_to_use, palette_to_use, palette_to_use])\n",
    "        col_opts.extend([{}, {}, {}])\n",
    "        col_legs.extend([False, False, False])\n",
    "        d.index = list(methods)\n",
    "        dhf = pd.concat([dhf, d], axis=1)\n",
    "\n",
    "    col_info_df = pd.DataFrame({\n",
    "        'id': list(dhf.columns),\n",
    "        'group': col_groups,\n",
    "        'name': col_names,\n",
    "        'geom': col_geoms,\n",
    "        'options': col_opts,\n",
    "        'palette': col_pals,\n",
    "        'legend': col_legs\n",
    "    })\n",
    "    col_info_df.index = col_info_df['id']\n",
    "\n",
    "    group_pal = np.tile(['palette1', 'palette2'], np.ceil(len(datasets)/2).astype(int))\n",
    "    col_groups_df = pd.DataFrame({\n",
    "        'Category': datasets,\n",
    "        'group': datasets,\n",
    "        'palette': group_pal\n",
    "    })\n",
    "    # col_groups_df.index = datasets\n",
    "    row_info_df = pd.DataFrame({'id': dhf['id'], 'group': ''}, index=dhf['id'])\n",
    "    row_groups_df = pd.DataFrame({'Group': [''], 'group': ['']})\n",
    "\n",
    "    norm = mpl.colors.Normalize(vmin=0, vmax=101, clip=True)\n",
    "    mapper = cm.ScalarMappable(norm=norm, cmap='Greys')\n",
    "    colours_bg = [mapper.to_rgba(i) for i in range(0, 101)]\n",
    "\n",
    "    palettes = pd.DataFrame.from_dict({\n",
    "        'palettes': ['overall', 'palette1', 'palette2'],\n",
    "        'colours': [colours_bg, 'Blues', 'Reds']\n",
    "    })\n",
    "\n",
    "    pos_args = {\n",
    "        'row_height': 1,\n",
    "        'row_space': 0.1,\n",
    "        'row_bigspace': 0.5,\n",
    "        'col_width': 1,\n",
    "        'col_space': 0.1,\n",
    "        'col_bigspace': 0.5,\n",
    "        'col_annot_offset': 3,\n",
    "        'col_annot_angle': 30,\n",
    "        'expand_xmin': 0,\n",
    "        'expand_xmax': 2,\n",
    "        'expand_ymin': 0,\n",
    "        'expand_ymax': 0\n",
    "    }\n",
    "\n",
    "    f = fh.funky_heatmap(\n",
    "        data=dhf,\n",
    "        column_info=col_info_df,\n",
    "        column_groups=col_groups_df,\n",
    "        row_info=row_info_df,\n",
    "        row_groups=row_groups_df,\n",
    "        palettes=palettes,\n",
    "        position_args=pos_args,\n",
    "        add_abc=False,\n",
    "        scale_column=False\n",
    "    )\n",
    "    \n",
    "    plt.title('A', x=.05, y=.88, size=36, weight='bold')\n",
    "\n",
    "    plt.savefig('plots/02_funky_heatmap.svg')\n",
    "    plt.savefig('plots/02_funky_heatmap.png', dpi=300, transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "plot_funky_heatmap(df_avg, datasets, methods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3.** Plotting $R_{NX}$ curves\n",
    "\n",
    "The $R_{NX}$ curve approximations (from which Local and Global SP are calculated) can be plotted directly for each dataset and method, and we can show the effect of de-noising as well.\n",
    "\n",
    "To show results on denoised inputs, set the argument `denoised` to True, to show results on raw inputs, set it to False.\n",
    "To use denoised inputs only for ViVAE (which it was designed for), set `denoised` to `'ViVAE'`.\n",
    "\n",
    "We display the plot, as well as saving it as a PNG and SVG file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_rnx_curves(rnx, nruns=5, denoised='ViVAE', fname=['./plots/03_rnx_curves.svg', './plots/03_rnx_curves.png']):\n",
    "\n",
    "    if denoised==True:\n",
    "        this_rnx = copy.deepcopy(rnx['uTrue'])\n",
    "    elif denoised==False:\n",
    "        this_rnx = copy.deepcopy(rnx['uFalse'])\n",
    "    elif denoised=='ViVAE': # use denoised inputs for ViVAE\n",
    "        this_rnx = copy.deepcopy(rnx['uFalse'])\n",
    "        k = list(this_rnx.keys())\n",
    "        for i, key in enumerate(k):\n",
    "            if key.startswith('ViVAE'):\n",
    "                this_rnx[key] = rnx['uTrue'][key]\n",
    "\n",
    "    mpl.rcParams['axes.linewidth'] = 0.2\n",
    "    fig, ax = plt.subplots(nrows=len(datasets), ncols=len(methods), figsize=(.8*len(methods), .7*len(datasets)), sharey=True, sharex=False, dpi=150)\n",
    "    fig.subplots_adjust(hspace=.5)\n",
    "    for i, dataset in enumerate(datasets):\n",
    "        for j, method in enumerate(methods):\n",
    "            curves = this_rnx[dataset][method]\n",
    "            for k, curve in enumerate(curves):\n",
    "                d = np.concatenate([np.array([0.]), curve, np.array([0.])])\n",
    "                ax[i][j].axhline(y=0, xmin=0, xmax=len(curve)+2, lw=.2, ls='--', color='gray')\n",
    "                ax[i][j].plot(range(len(curve)+2), d, lw=.5)\n",
    "                ax[i][j].xaxis.set_major_formatter(mpl.ticker.FuncFormatter(lambda x, pos: ('{:,.0f}'.format(x/1000) + 'K') if x>0 else '0'))\n",
    "                ax[i][j].tick_params(axis='both', labelsize=5)\n",
    "\n",
    "    pad = 5\n",
    "    for a, col in zip(ax[0], methods):\n",
    "        a.annotate(col, xy=(.5, 1), xytext=(0, pad), xycoords='axes fraction', textcoords='offset points', size=6, weight='bold', ha='center', va='baseline')\n",
    "    for a, row in zip(ax[:,0], datasets):\n",
    "        a.annotate(row, xy=(0, .5), xytext=(0, 0), xycoords=a.yaxis.label, textcoords='offset points', size=6, weight='bold', ha='right', va='center')\n",
    "\n",
    "    #fig.suptitle(r'$R_{NX}$ curve approximations', y=0.94, size=10)\n",
    "    fig.suptitle('B', x=-.02, y=.9, size=12, weight='bold')\n",
    "    fig.show()\n",
    "    if fname is not None:\n",
    "        if isinstance(fname, list) and len(fname)>0:\n",
    "            for f in fname:\n",
    "                if f.endswith('.png'):\n",
    "                    fig.savefig(f, bbox_inches='tight', dpi=300)\n",
    "                else:\n",
    "                    fig.savefig(f, bbox_inches='tight')\n",
    "        elif isinstance(fname, str):\n",
    "                fig.savefig(fname, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "plot_rnx_curves(rnx, nruns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.** Plotting labelled embeddings\n",
    "\n",
    "We create a plot of embeddings of all datasets by all tested methods, with points coloured by labelled cell populations.\n",
    "Legends for the colour scheme will be saved separately for each dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_embeddings(datasets, methods, denoised='ViVAE', seed=1, palette=palette,\n",
    "        #fname=['./plots/03_embeddings.svg', './plots/03_embeddings.png']\n",
    "        fname_embedding='./plots/04_embeddings.png',\n",
    "        fpath_legends='./plots/04_legends'\n",
    "    ):\n",
    "\n",
    "    ## Plot embeddings\n",
    "\n",
    "    fig, ax = plt.subplots(nrows=len(datasets), ncols=len(methods), figsize=(1.4*len(methods), 1.4*len(datasets)), dpi=300)\n",
    "    fig.subplots_adjust(hspace=.35, wspace=.4)\n",
    "    handles = []\n",
    "    labels = []\n",
    "    for i, dataset in enumerate(datasets):\n",
    "        labs = np.load(f'./data/{dataset}_labels.npy', allow_pickle=True)\n",
    "        unas = np.load(f'./data/{dataset}_unassigned.npy', allow_pickle=True).item()\n",
    "        idcs0 = np.array([])\n",
    "        idcs1 = np.arange(len(labs))\n",
    "        s = 5e-2 if len(labs)>30000 else 1e-1\n",
    "        if unas is not None:\n",
    "            idcs0 = np.where(labs == unas)[0]\n",
    "            idcs1 = np.delete(idcs1, idcs0)\n",
    "            labs = np.delete(labs, idcs0)\n",
    "        for j, method in enumerate(methods):\n",
    "            if denoised==False or denoised==True:\n",
    "                emb = np.load(f'./results/{dataset}_{method}_z2_u{denoised}/emb_seed{seed}.npy', allow_pickle=True)\n",
    "            elif denoised=='ViVAE':\n",
    "                if method.startswith('ViVAE'):\n",
    "                    emb = np.load(f'./results/{dataset}_{method}_z2_uTrue/emb_seed1.npy', allow_pickle=True)\n",
    "                else:\n",
    "                    emb = np.load(f'./results/{dataset}_{method}_z2_uFalse/emb_seed1.npy', allow_pickle=True)\n",
    "            \n",
    "            if len(idcs0)>0:\n",
    "                ax[i][j].scatter(emb[idcs0,0], emb[idcs0,1], s=s, c='#bfbfbf', alpha=1., marker='o', linewidths=0)\n",
    "            idx_pop = 0\n",
    "            for pop in np.unique(labs):\n",
    "                idcs = np.where(labs == pop)[0]\n",
    "                ax[i][j].scatter(emb[idcs1[idcs],0], emb[idcs1[idcs],1], label=pop, s=s, c=palette[idx_pop], alpha=1., marker='o', linewidths=0)\n",
    "                ax[i][j].tick_params(axis='both', labelsize=5)\n",
    "                idx_pop += 1\n",
    "            ax[i][j].axis('equal')\n",
    "        this_handles, this_labels = ax[i][0].get_legend_handles_labels()\n",
    "        handles.append(this_handles)\n",
    "        labels.append(this_labels)\n",
    "    pad = 5\n",
    "    for a, col in zip(ax[0], methods):\n",
    "        a.annotate(col, xy=(.5, 1), xytext=(0, pad), xycoords='axes fraction', textcoords='offset points', size=6, weight='bold', ha='center', va='baseline')\n",
    "    for a, row in zip(ax[:,0], datasets):\n",
    "        a.annotate(row, xy=(0, .5), xytext=(0, 0), xycoords=a.yaxis.label, textcoords='offset points', size=6, weight='bold', ha='right', va='center')\n",
    "    fig.suptitle('A', x=.03, y=.89, size=12, weight='bold')\n",
    "    if fname_embedding is not None:\n",
    "        if isinstance(fname_embedding, list) and len(fname_embedding)>0:\n",
    "            for f in fname_embedding:\n",
    "                if f.endswith('.png'):\n",
    "                    fig.savefig(f, bbox_inches='tight', dpi=300)\n",
    "                else:\n",
    "                    fig.savefig(f, bbox_inches='tight')\n",
    "        elif isinstance(fname_embedding, str):\n",
    "                fig.savefig(fname_embedding, bbox_inches='tight')\n",
    "\n",
    "        ## Plot legends\n",
    "\n",
    "        if not os.path.exists(fpath_legends):\n",
    "            os.mkdir(fpath_legends)\n",
    "        for i, dataset in enumerate(datasets):\n",
    "            plt.clf()\n",
    "            plt.axis(False)\n",
    "            l = plt.legend(handles[i], labels[i], loc='center', title=f'{dataset}', title_fontproperties={'weight': 'bold'})\n",
    "            for lh in l.legend_handles: \n",
    "                lh.set_alpha(1.)\n",
    "                lh._sizes = [50]\n",
    "            l._legend_box.align = 'center'\n",
    "            plt.savefig(os.path.join(fpath_legends, f'04_legend_{dataset}.png'), bbox_inches='tight', dpi=300)\n",
    "            plt.savefig(os.path.join(fpath_legends, f'04_legend_{dataset}.svg'), bbox_inches='tight', dpi=300)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "plot_embeddings(datasets, methods)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ViVAE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
