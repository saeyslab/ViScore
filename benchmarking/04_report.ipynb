{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reporting ViScore benchmark results\n",
    "\n",
    "Once our benchmark is completed, we will want to report the results somehow.\n",
    "We can do this by creating tables of results and informative plots.\n",
    "\n",
    "We will need a Python environment with ViScore, its dependencies, `funkyheatmappy` and `adjustText`.\n",
    "`funkyheatmappy` and `adjustText` are installed using the following command in shell:\n",
    "\n",
    "```\n",
    "pip install git+https://github.com/funkyheatmap/funkyheatmappy.git\n",
    "pip install git+https://github.com/Phlya/adjustText.git\n",
    "```\n",
    "\n",
    "We assume that you followed instructions in `README.md` for designing and running your benchmark.\n",
    "In accordance with that, we assume that\n",
    "\n",
    "* results of benchmark are stored in `./results`\n",
    "* all datasets listed in `./datasets.txt` were used\n",
    "* all methods listed in `./config.json` were used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import copy\n",
    "import pandas as pd\n",
    "import funkyheatmappy as fh\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib as mpl\n",
    "import matplotlib.patheffects as pe\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "from adjustText import adjust_text\n",
    "from scipy.spatial import ConvexHull\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "palette = [\n",
    "            '#000000', '#1CE6FF', '#FF34FF', '#FF4A46', '#008941', '#006FA6', '#A30059',\n",
    "            '#7A4900', '#0000A6', '#63FFAC', '#B79762', '#004D43', '#8FB0FF', '#997D87',\n",
    "            '#5A0007', '#809693', '#1B4400', '#4FC601', '#3B5DFF', '#4A3B53', '#FF2F80',\n",
    "            '#61615A', '#BA0900', '#6B7900', '#00C2A0', '#FFAA92', '#FF90C9', '#B903AA', '#D16100',\n",
    "            '#DDEFFF', '#000035', '#7B4F4B', '#A1C299', '#300018', '#0AA6D8', '#013349', '#00846F',\n",
    "            '#372101', '#FFB500', '#C2FFED', '#A079BF', '#CC0744', '#C0B9B2', '#C2FF99', '#001E09',\n",
    "            '#00489C', '#6F0062', '#0CBD66', '#EEC3FF', '#456D75', '#B77B68', '#7A87A1', '#788D66',\n",
    "            '#885578', '#FAD09F', '#FF8A9A', '#D157A0', '#BEC459', '#456648', '#0086ED', '#886F4C',\n",
    "            '#34362D', '#B4A8BD', '#00A6AA', '#452C2C', '#636375', '#A3C8C9', '#FF913F', '#938A81',\n",
    "            '#575329', '#00FECF', '#B05B6F', '#8CD0FF', '#3B9700', '#04F757', '#C8A1A1', '#1E6E00',\n",
    "            '#7900D7', '#A77500', '#6367A9', '#A05837', '#6B002C', '#772600', '#D790FF', '#9B9700',\n",
    "            '#549E79', '#FFF69F', '#201625', '#72418F', '#BC23FF', '#99ADC0', '#3A2465', '#922329',\n",
    "            '#5B4534', '#FDE8DC', '#404E55', '#0089A3', '#CB7E98', '#A4E804', '#324E72', '#6A3A4C'\n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **0.** Collecting results\n",
    "\n",
    "We start by aggregating quantitative results.\n",
    "\n",
    "* The `rnx`, `sl` and `sg` dictionaries will contain denoised and non-denoised RNX, Local SP and Global SP values for all method-dataset combinations.\n",
    "* The `df_all` dataframe will contain all Local SP, Global SP, Balanced SP (using either geometric mean or harmonic mean) and average xNPE values per method-dataset-denoising combination, for each run (random seed).\n",
    "* The `df_agg` dataframe will contain mean and standard-deviation values, aggregating across runs.\n",
    "\n",
    "<hr>\n",
    "\n",
    "The first step is to determine the datasets and methods used in this benchmark.\n",
    "**If this is anything other than what is indicated in `./datasets.txt` and `./config.json`, you will need to adapt this manually.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname_datasets = './datasets.txt'\n",
    "with open(fname_datasets, 'r') as f:\n",
    "  datasets = [line.strip() for line in f.readlines()]\n",
    "fname_config = './config.json'\n",
    "with open(fname_config, encoding='utf-8') as f:\n",
    "    conf = json.load(f)\n",
    "methods = list(conf['methods'].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to specify:\n",
    "\n",
    "* Which target dimensionality we are working with (`zdim`).\n",
    "\n",
    "* How many repeated runs of each set-up we have (`nruns`).\n",
    "\n",
    "* Whether we are working with results for denoised inputs. The `denoised` variable can be `False` (use results on non-denoised data), `True` (use results on denosied data) or `'ViVAE'` (only use denoised data results for ViVAE: this is what ViVAE was designed for).\n",
    "**Crucially, this does not mean ViVAE is evaluated against denoised inputs (this would be an unfair comparison): it is evaluated the same way all other methods are.**\n",
    "\n",
    "* Whether to use `'harmonic_mean'` or `'geometric_mean'` for computing balanced (local-global) structure preservation (there is a case to be made for both)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zdim = 2\n",
    "nruns = 5\n",
    "denoised = 'ViVAE'\n",
    "balanced_measure = 'geometric_mean'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##AD HOC\n",
    "\n",
    "methods = ['PCA', 'UMAP', 'DensMAP', 'tSNE', 'PaCMAP', 'TriMap', 'SQuad-MDS']\n",
    "datasets = ['Qiu', 'Reed', 'Suo', 'Strati', 'TabulaMuris', 'TabulaSapiens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_dicts(datasets, methods, zdim=zdim, nruns=nruns):\n",
    "    def _collect_dict(denoised=False):\n",
    "        rnx = {}\n",
    "        sl = {}\n",
    "        sg = {}\n",
    "        for dataset in datasets:\n",
    "            d_rnx = {}\n",
    "            d_sl = {}\n",
    "            d_sg = {}\n",
    "            for method in methods:\n",
    "                m_rnx = [np.load(f'results/{dataset}_{method}_z{zdim}_u{denoised}/rnx_curve_seed{seed}.npy', allow_pickle=True) for seed in range(1, nruns+1)]        \n",
    "                m_sl = [np.load(f'results/{dataset}_{method}_z{zdim}_u{denoised}/sp_local_seed{seed}.npy', allow_pickle=True) for seed in range(1, nruns+1)]\n",
    "                m_sg = [np.load(f'results/{dataset}_{method}_z{zdim}_u{denoised}/sp_global_seed{seed}.npy', allow_pickle=True) for seed in range(1, nruns+1)]\n",
    "                d_rnx.update({method: m_rnx})\n",
    "                d_sl.update({method: m_sl})\n",
    "                d_sg.update({method: m_sg})\n",
    "            rnx.update({dataset: d_rnx})\n",
    "            sl.update({dataset: d_sl})\n",
    "            sg.update({dataset: d_sg})\n",
    "        return rnx, sl, sg\n",
    "    rnx = {}\n",
    "    sl = {}\n",
    "    sg = {}\n",
    "    rnx_uFalse, sl_uFalse, sg_uFalse = _collect_dict(denoised=False)\n",
    "    rnx_uTrue, sl_uTrue, sg_uTrue = _collect_dict(denoised=True)\n",
    "    rnx.update({'uFalse': rnx_uFalse})\n",
    "    rnx.update({'uTrue':  rnx_uTrue})\n",
    "    sl.update({'uFalse': sl_uFalse})\n",
    "    sl.update({'uTrue':  sl_uTrue})\n",
    "    sg.update({'uFalse': sg_uFalse})\n",
    "    sg.update({'uTrue':  sg_uTrue})\n",
    "    return rnx, sl, sg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_df_avg(\n",
    "    datasets,\n",
    "    methods,\n",
    "    nruns=5,\n",
    "    zdim=zdim,\n",
    "    balanced_measure=balanced_measure, # geometric_mean or harmonic_mean (F-score) to calculate balance between Local and Global SP: debatable which one is more valid\n",
    "    wide=True\n",
    "): # wide or long format\n",
    "    res = []\n",
    "    for dataset in datasets:\n",
    "        for method in methods:\n",
    "            for denoised in [False, True]:\n",
    "                fpath    = os.path.join('results', f'{dataset}_{method}_z{zdim}_u{denoised}')\n",
    "\n",
    "                localsp  = np.array([np.load(os.path.join(fpath, f'sp_local_seed{seed}.npy'), allow_pickle=True) for seed in range(1, nruns+1)])\n",
    "                globalsp = np.array([np.load(os.path.join(fpath, f'sp_global_seed{seed}.npy'), allow_pickle=True) for seed in range(1, nruns+1)])\n",
    "                if balanced_measure=='geometric_mean':\n",
    "                    balsp = np.array([np.sqrt(localsp[i]*globalsp[i]) for i in range(nruns)])\n",
    "                elif balanced_measure=='harmonic_mean':\n",
    "                    balsp = np.array([2*(localsp[i]*globalsp[i])/(localsp[i]+globalsp[i]) for i in range(nruns)])\n",
    "                xnpe     = np.array([np.load(os.path.join(fpath, f'xnpe_seed{seed}.npy'), allow_pickle=True).tolist() for seed in range(1, nruns+1)])\n",
    "                xnpemean = np.array([np.mean(list(x.values())) for x in xnpe])\n",
    "                if wide:\n",
    "                    res.append([\n",
    "                        dataset,\n",
    "                        method,\n",
    "                        zdim,\n",
    "                        denoised,\n",
    "                        np.mean(localsp), np.std(localsp),\n",
    "                        np.mean(globalsp), np.std(globalsp), \n",
    "                        np.mean(balsp), np.std(balsp),\n",
    "                        np.mean(xnpemean), np.std(xnpemean)\n",
    "                    ])\n",
    "                else:\n",
    "                    res.append([dataset, method, zdim, denoised, 'LocalSP', 'Mean', np.mean(localsp)])\n",
    "                    res.append([dataset, method, zdim, denoised, 'LocalSP', 'SD', np.std(localsp)])\n",
    "                    res.append([dataset, method, zdim, denoised, 'GlobalSP', 'Mean', np.mean(globalsp)])\n",
    "                    res.append([dataset, method, zdim, denoised, 'GlobalSP', 'SD', np.std(globalsp)])\n",
    "                    res.append([dataset, method, zdim, denoised, 'BalancedSP', 'Mean', np.mean(balsp)])\n",
    "                    res.append([dataset, method, zdim, denoised, 'BalancedSP', 'SD', np.std(balsp)])\n",
    "                    res.append([dataset, method, zdim, denoised, 'xNPEMean', 'Mean', np.mean(xnpemean)])\n",
    "                    res.append([dataset, method, zdim, denoised, 'xNPEMean', 'SD', np.std(xnpemean)])\n",
    "\n",
    "    if wide:\n",
    "        df_avg = pd.DataFrame(columns=[\n",
    "            'Dataset', 'Method', 'zdim', 'Denoised',\n",
    "            'LocalSP_Mean', 'LocalSP_SD',\n",
    "            'GlobalSP_Mean', 'GlobalSP_SD',\n",
    "            'BalancedSP_Mean', 'BalancedSP_SD',\n",
    "            'xNPEMean_Mean', 'xNPEMean_SD'\n",
    "        ], data = res)\n",
    "    else:\n",
    "        df_avg = pd.DataFrame(columns=[\n",
    "            'Dataset', 'Method', 'zdim', 'Denoised',\n",
    "            'id', 'stat', 'value'\n",
    "        ], data = res)\n",
    "\n",
    "    return df_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_df_all(\n",
    "    datasets,\n",
    "    methods,\n",
    "    nruns=5,\n",
    "    zdim=zdim,\n",
    "    balanced_measure=balanced_measure, # geom_mean or f1 to calculate balance between Local and Global SP: debatable which one is more valid\n",
    "    wide=True\n",
    "): # wide or long format\n",
    "    res = []\n",
    "    for dataset in datasets:\n",
    "        for method in methods:\n",
    "            for denoised in [False, True]:\n",
    "                fpath    = os.path.join('results', f'{dataset}_{method}_z{zdim}_u{denoised}')\n",
    "\n",
    "                localsp  = np.array([np.load(os.path.join(fpath, f'sp_local_seed{seed}.npy'), allow_pickle=True) for seed in range(1, nruns+1)])\n",
    "                globalsp = np.array([np.load(os.path.join(fpath, f'sp_global_seed{seed}.npy'), allow_pickle=True) for seed in range(1, nruns+1)])\n",
    "                if balanced_measure=='geometric_mean':\n",
    "                    balsp = np.array([np.sqrt(localsp[i]*globalsp[i]) for i in range(nruns)])\n",
    "                elif balanced_measure=='harmonic_mean':\n",
    "                    balsp = np.array([2*(localsp[i]*globalsp[i])/(localsp[i]+globalsp[i]) for i in range(nruns)])\n",
    "                xnpe     = np.array([np.load(os.path.join(fpath, f'xnpe_seed{seed}.npy'), allow_pickle=True).tolist() for seed in range(1, nruns+1)])\n",
    "                xnpemean = np.array([np.mean(list(x.values())) for x in xnpe])\n",
    "\n",
    "                if wide:\n",
    "                    for i in range(len(localsp)):\n",
    "                        res.append([dataset, method, zdim, denoised, localsp[i], globalsp[i], balsp[i], xnpemean[i]])\n",
    "                else:\n",
    "                    for i in range(len(localsp)):\n",
    "                        res.append([dataset, method, zdim, denoised, 'LocalSP', localsp[i]])\n",
    "                    for i in range(len(globalsp)):\n",
    "                        res.append([dataset, method, zdim, denoised, 'GlobalSP', globalsp[i]])\n",
    "                    for i in range(len(balsp)):\n",
    "                        res.append([dataset, method, zdim, denoised, 'BalancedSP', balsp[i]])\n",
    "                    for i in range(len(xnpemean)):\n",
    "                        res.append([dataset, method, zdim, denoised, 'xNPEMean', xnpemean[i]])\n",
    "\n",
    "    if wide:\n",
    "        df_all = pd.DataFrame(columns=[\n",
    "            'Dataset', 'Method', 'zdim', 'Denoised', 'LocalSP', 'GlobalSP', 'BalancedSP', 'xNPEMean'\n",
    "        ], data = res)\n",
    "    else:\n",
    "        df_all = pd.DataFrame(columns=[\n",
    "            'Dataset', 'Method', 'zdim', 'Denoised', 'id', 'value'\n",
    "        ], data = res)\n",
    "\n",
    "    return df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_denoised_mask(df, denoised):\n",
    "    if denoised==True or denoised==False:\n",
    "        mask_denoised = np.array(df['Denoised']==denoised)\n",
    "    elif denoised=='ViVAE':\n",
    "        mask_denoised = np.logical_or.reduce([\n",
    "            np.logical_and.reduce([df['Denoised']==True, pd.Series([x.startswith('ViVAE') for x in df['Method']])]),\n",
    "            np.logical_and.reduce([df['Denoised']==False, pd.Series([not x.startswith('ViVAE') for x in df['Method']])])\n",
    "        ])\n",
    "    return mask_denoised"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's collect the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnx, sl, sg = collect_dicts(datasets, methods)\n",
    "df_avg = collect_df_avg(datasets, methods, wide=True)\n",
    "df_all = collect_df_all(datasets, methods, wide=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('./report'):\n",
    "    os.mkdir('./report')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1.** Plotting structure-preservation values\n",
    "\n",
    "We will plot the Local, Global and Balanced SP using scatterplots with errorbars for separate categories and a scatterplot showing the Local-Global trade-off.\n",
    "\n",
    "<hr>\n",
    "\n",
    "First, the separate plotting of Local, Global and Balanced SP, using points with error bars (mean and standard deviation), separately also for each dataset.\n",
    "This is not the easiest plot to look at, but we make it for the sake of completeness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_separate_sp(datasets, methods, df_all, denoised='ViVAE'):\n",
    "    mpl.rcParams['axes.linewidth'] = 0.1\n",
    "    fname = ['./report/01_sp_separate.png', './report/01_sp_separate.svg']\n",
    "    fig, ax = plt.subplots(nrows=len(datasets), ncols=3, figsize=(4.5, .9*len(datasets)), sharey=True, dpi=150)\n",
    "    fig.subplots_adjust(hspace=.5)\n",
    "    for i, dataset in enumerate(datasets):\n",
    "\n",
    "        for j, method in enumerate(methods):\n",
    "\n",
    "            mask_dataset = df_all['Dataset']==dataset\n",
    "            mask_method = df_all['Method']==method\n",
    "            mask_denoised = get_denoised_mask(df_all, denoised=denoised)\n",
    "            mask = np.logical_and.reduce([mask_dataset, mask_method, mask_denoised])\n",
    "\n",
    "            ## Local SP\n",
    "            d = df_all['LocalSP'][mask]\n",
    "            ax[i][0].errorbar(x=np.mean(d), y=j, xerr=np.std(d), label=method, color=palette[j], markersize=1.8, alpha=.9, fmt='o', linewidth=1.6)\n",
    "            ax[i][0].grid(visible=True, axis='x', ls='--')\n",
    "\n",
    "            ## Global SP\n",
    "            d = df_all['GlobalSP'][mask]\n",
    "            ax[i][1].errorbar(x=np.mean(d), y=j, xerr=np.std(d), label=method, color=palette[j], markersize=1.8, alpha=.9, fmt='o', linewidth=1.6)\n",
    "            ax[i][1].grid(visible=True, axis='x', ls='--')\n",
    "\n",
    "            ## Balanced SP\n",
    "            d = df_all['BalancedSP'][mask]\n",
    "            ax[i][2].errorbar(x=np.mean(d), y=j, xerr=np.std(d), label=method, color=palette[j], markersize=1.8, alpha=.9, fmt='o', linewidth=1.6)\n",
    "            ax[i][2].grid(visible=True, axis='x', ls='--')\n",
    "\n",
    "        ax[i][0].set_yticks(ticks=range(len(methods)), labels=methods)\n",
    "        ax[i][1].yaxis.set_tick_params(left=False)\n",
    "        ax[i][2].yaxis.set_tick_params(left=False)\n",
    "        for j in [0, 1, 2]:\n",
    "            ax[i][j].tick_params(axis='both', labelsize=5)\n",
    "    pad = 5\n",
    "    for a, row in zip(ax[:,0], datasets):\n",
    "            a.annotate(row, xy=(0, .5), xytext=(0, 0), xycoords=a.yaxis.label, textcoords='offset points', size=6, weight='bold', ha='right', va='center')\n",
    "    for a, col in zip(ax[0], ['Local SP', 'Global SP', 'Balanced SP']):\n",
    "            a.annotate(col, xy=(.4, 1), xytext=(0, pad), xycoords='axes fraction', textcoords='offset points', size=6, weight='bold', ha='left', rotation=45, va='baseline')\n",
    "    fig.suptitle('A', x=-.1, y=.97, size=12, weight='bold')\n",
    "    if fname is not None:\n",
    "        if isinstance(fname, list) and len(fname)>0:\n",
    "            for f in fname:\n",
    "                if f.endswith('.png'):\n",
    "                    fig.savefig(f, bbox_inches='tight', dpi=300, transparent=True)\n",
    "                else:\n",
    "                    fig.savefig(f, bbox_inches='tight', transparent=True)\n",
    "        elif isinstance(fname, str):\n",
    "                fig.savefig(fname, bbox_inches='tight', transparent=True)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_separate_sp(datasets, methods, df_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, we plot the trade-off/balance between Local and Global by the means of a scatter plot.\n",
    "We do this dataset by dataset, using the *x*-axis for Local SP, the *y*-axis for Global SP.\n",
    "Each method/set-up gets a point place where the mean values are for each measure, and then an ellipse centered around the mean gets drawn to indicate standard deviation for both measures.\n",
    "The ellipses can turn out barely visible or invisible in case the standard deviation is small relative to the axis range.\n",
    "We use text labels for each point & ellipse, to indicate which method is which.\n",
    "\n",
    "Additionally, we make one plot where we aggregate results across all datasets, to indicate which method falls into which quadrant of the plot generally (low-low, low-high, high-low or high-high with respect to Local and Global SP).\n",
    "\n",
    "If `pareto` is set to True, we put an asterisk next to the name of each method that lies on the Pareto front of the results for a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sp_tradeoffs(datasets, methods, df_avg, pareto=True, denoised='ViVAE'):\n",
    "    for joint in [False, True]:\n",
    "        fpath_plots = './report/03_sp_tradeoffs/'\n",
    "        if not os.path.exists(fpath_plots):\n",
    "            os.mkdir(fpath_plots)\n",
    "\n",
    "        if joint:\n",
    "            fig, ax = plt.subplots(figsize=(1.8,1.8), dpi=150)\n",
    "        for i, dataset in enumerate(datasets):\n",
    "            if not joint:\n",
    "                fig, ax = plt.subplots(figsize=(1.8,1.8), dpi=150)\n",
    "            xlims = np.array([np.min(df_all['LocalSP'])-.05, np.max(df_all['LocalSP'])+.05])\n",
    "            ylims = np.array([np.min(df_all['GlobalSP'])-.05, np.max(df_all['GlobalSP'])+.05])\n",
    "            \n",
    "            mask_dataset = df_avg['Dataset']==dataset\n",
    "            xcoords = []\n",
    "            ycoords = []\n",
    "            ax.set_xlim(xlims)\n",
    "            ax.set_ylim(ylims)\n",
    "            for j, method in enumerate(methods):\n",
    "                mask_method = df_avg['Method']==method\n",
    "                if denoised==True or denoised==False:\n",
    "                    mask_denoised = df_avg['Denoised']==denoised\n",
    "                elif denoised=='ViVAE':\n",
    "                    mask_denoised = df_avg['Denoised']==True if method.startswith('ViVAE') else df_avg['Denoised']==False\n",
    "                mask = np.logical_and.reduce([mask_dataset, mask_method, mask_denoised])\n",
    "\n",
    "                mu_local     = df_avg['LocalSP_Mean'][mask]\n",
    "                mu_global    = df_avg['GlobalSP_Mean'][mask]\n",
    "                sigma_local  = df_avg['LocalSP_SD'][mask]\n",
    "                sigma_global = df_avg['GlobalSP_SD'][mask]\n",
    "\n",
    "                xcoords.append(mu_local)    \n",
    "                ycoords.append(mu_global)\n",
    "\n",
    "                if not joint:\n",
    "                    ellipse = mpl.patches.Ellipse(xy=(mu_local, mu_global), width=sigma_local, height=sigma_global, color=palette[j], alpha=.3)\n",
    "\n",
    "                    ax.add_patch(ellipse)\n",
    "                \n",
    "                    ax.vlines(x=mu_local, ymin=np.repeat(0., len(mu_global)), ymax=mu_global, color='gray', lw=.5, linestyles='dashed', zorder=1)\n",
    "                    ax.hlines(y=mu_global, xmin=np.repeat(0., len(mu_local)), xmax=mu_local, color='gray', lw=.5, linestyles='dashed', zorder=1)\n",
    "                \n",
    "                ax.scatter(x=mu_local, y=mu_global, s=25 if joint else 8, marker='.', color=palette[j], label=method, alpha=.8, zorder=2)\n",
    "\n",
    "                ax.set_xlabel('Local SP', fontsize=5)\n",
    "                ax.set_ylabel('Global SP', fontsize=5)\n",
    "\n",
    "            if not joint:\n",
    "\n",
    "                method_labs = copy.deepcopy(methods)\n",
    "                if pareto:\n",
    "                    scores = np.hstack([np.array(xcoords), np.array(ycoords)])\n",
    "                    costs = 1-scores\n",
    "                    eff = np.ones(costs.shape[0], dtype=bool)\n",
    "                    for i, c in enumerate(costs):\n",
    "                        eff[i] = np.all(np.any(costs[:i] > c, axis=1)) and np.all(np.any(costs[i+1:] > c, axis=1))\n",
    "                    pareto_idcs = np.where(eff)[0]\n",
    "                    for k in pareto_idcs:\n",
    "                        method_labs[k] = f'{method_labs[k]}*'\n",
    "\n",
    "                texts = []\n",
    "                for k, (x, y, s) in enumerate(zip(xcoords, ycoords, method_labs)):\n",
    "                    texts.append(plt.text(x, y, s, color=palette[k], size=5, path_effects=[pe.withStroke(linewidth=.2, foreground='black')]))\n",
    "                adjust_text(texts, force_points=10., force_text=30., arrowprops=dict(arrowstyle='-', color='lightgray', alpha=.5, lw=.8))\n",
    "            \n",
    "            if not joint:\n",
    "                ax.annotate(dataset, xy=(-.8, .5), xytext=(0, 0), xycoords=ax.yaxis.label, textcoords='offset points', size=6, weight='bold', ha='right', va='center')\n",
    "                ax.tick_params(axis='both', labelsize=5)\n",
    "                fig.savefig(f'report/03_sp_tradeoffs/03_sp_tradeoff_{dataset}.svg', bbox_inches='tight', transparent=True)\n",
    "                fig.savefig(f'report/03_sp_tradeoffs/03_sp_tradeoff_{dataset}.png', bbox_inches='tight', dpi=300, transparent=True)\n",
    "\n",
    "        if joint:\n",
    "            ax.annotate('All datasets', xy=(-.8, .5), xytext=(0, 0), xycoords=ax.yaxis.label, textcoords='offset points', size=6, weight='bold', ha='right', va='center')\n",
    "            ax.tick_params(axis='both', labelsize=5)\n",
    "\n",
    "            fig.savefig('report/03_sp_tradeoffs/03_sp_tradeoff_JOINT.svg', bbox_inches='tight', transparent=True)\n",
    "            fig.savefig('report/03_sp_tradeoffs/03_sp_tradeoff_JOINT.png', bbox_inches='tight', dpi=300, transparent=True)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "plot_sp_tradeoffs(datasets, methods, df_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.** Plotting structure preservation in a heatmap\n",
    "\n",
    "We use a [funky heatmap](https://funkyheatmap.github.io/funkyheatmap/) to plot Local, Global and Balanced SP.\n",
    "This is a popular visualisation method used often in benchmarking reports.\n",
    "In order for things to work correctly for us, we need to monkey-patch some functions in the `funkyheatmappy` module.\n",
    "\n",
    "**The plotted values are min-max scaled for each of the 3 categories.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Monkey-patch (long)\n",
    "\n",
    "def calculate_positions(\n",
    "    data,\n",
    "    column_info,\n",
    "    row_info,\n",
    "    column_groups,\n",
    "    row_groups,\n",
    "    palettes,\n",
    "    position_args,\n",
    "    scale_column,\n",
    "    add_abc,\n",
    "):\n",
    "    row_height = position_args[\"row_height\"]\n",
    "    row_space = position_args[\"row_space\"]\n",
    "    row_bigspace = position_args[\"row_bigspace\"]\n",
    "    col_width = position_args[\"col_width\"]\n",
    "    col_space = position_args[\"col_space\"]\n",
    "    col_bigspace = position_args[\"col_bigspace\"]\n",
    "    col_annot_offset = position_args[\"col_annot_offset\"]\n",
    "    col_annot_angle = position_args[\"col_annot_angle\"]\n",
    "\n",
    "    # Determine row positions\n",
    "    if not \"group\" in row_info.columns or all(pd.isna(row_info[\"group\"])):\n",
    "        row_info[\"group\"] = \"\"\n",
    "        row_groups = pd.DataFrame({\"group\": [\"\"]})\n",
    "        plot_row_annotation = False\n",
    "    else:\n",
    "        plot_row_annotation = True\n",
    "\n",
    "    row_pos = fh.calculate_row_positions.calculate_row_positions(\n",
    "        row_info=row_info,\n",
    "        row_height=row_height,\n",
    "        row_space=row_space,\n",
    "        row_bigspace=row_bigspace,\n",
    "    )\n",
    "\n",
    "    # Determine column positions\n",
    "    if not \"group\" in column_info.columns or all(pd.isna(column_info[\"group\"])):\n",
    "        column_info[\"group\"] = \"\"\n",
    "        column_groups = pd.DataFrame({\"group\": [\"\"]})\n",
    "        plot_column_annotation = False\n",
    "    else:\n",
    "        plot_column_annotation = True\n",
    "\n",
    "    column_pos = fh.calculate_column_positions.calculate_column_positions(\n",
    "        column_info=column_info, col_space=col_space, col_bigspace=col_bigspace\n",
    "    )\n",
    "\n",
    "    # Process data\n",
    "    data_processor = fh.make_data_processor.make_data_processor(\n",
    "        data=data,\n",
    "        column_pos=column_pos,\n",
    "        row_pos=row_pos,\n",
    "        scale_column=scale_column,\n",
    "        palette_list=palettes,\n",
    "    )\n",
    "\n",
    "    def circle_fun(dat):\n",
    "        dat = dat.assign(x0=dat[\"x\"], y0=dat[\"y\"], r=row_height / 2 * dat[\"value\"])\n",
    "        return dat\n",
    "\n",
    "    circle_data = data_processor(\"circle\", circle_fun)\n",
    "\n",
    "    def rect_fun(dat):\n",
    "        return dat\n",
    "\n",
    "    rect_data = data_processor(\"rect\", rect_fun)\n",
    "\n",
    "    def funkyrect_fun(dat):\n",
    "        result = pd.concat(\n",
    "            [\n",
    "                fh.score_to_funkyrectangle.score_to_funkyrectangle(\n",
    "                    xmin=row[\"xmin\"],\n",
    "                    xmax=row[\"xmax\"],\n",
    "                    ymin=row[\"ymin\"],\n",
    "                    ymax=row[\"ymax\"],\n",
    "                    value=row[\"value\"],\n",
    "                    midpoint=0.8,\n",
    "                )\n",
    "                for _, row in dat[[\"xmin\", \"xmax\", \"ymin\", \"ymax\", \"value\"]].iterrows()\n",
    "            ]\n",
    "        )\n",
    "        return result\n",
    "\n",
    "    funkyrect_data = data_processor(\"funkyrect\", funkyrect_fun)\n",
    "\n",
    "    def bar_fun(dat):\n",
    "        dat = fh.add_column_if_missing.add_column_if_missing(dat, hjust=0)\n",
    "        dat = dat.assign(\n",
    "            xmin=dat[\"xmin\"] + (1 - dat[\"value\"]) * dat[\"xwidth\"] * dat[\"hjust\"],\n",
    "            xmax=dat[\"xmax\"] - (1 - dat[\"value\"]) * dat[\"xwidth\"] * (1 - dat[\"hjust\"]),\n",
    "        )\n",
    "        return dat\n",
    "\n",
    "    bar_data = data_processor(\"bar\", bar_fun)\n",
    "\n",
    "    def barguides_fun(dat):\n",
    "        dat = ((dat.groupby(\"column_id\").first())[[\"xmin\", \"xmax\"]]).melt(\n",
    "            var_name=\"col\", value_name=\"x\"\n",
    "        )\n",
    "        dat = dat.assign(xend=dat[\"x\"])[[\"x\", \"xend\"]]\n",
    "        cols_to_add = pd.DataFrame({\"y\": row_pos[\"ymin\"], \"yend\": row_pos[\"ymax\"]})\n",
    "        result = (\n",
    "            pd.merge(dat.assign(key=1), cols_to_add.assign(key=1), on=\"key\")\n",
    "            .drop(\"key\", axis=1)\n",
    "            .sort_values([\"x\", \"xend\"])\n",
    "            .reset_index(drop=True)\n",
    "            .drop_duplicates()\n",
    "            .assign(palette=np.nan, value=np.nan)\n",
    "        )\n",
    "        return result\n",
    "\n",
    "    segment_data = data_processor(\"bar\", barguides_fun).assign(\n",
    "        colour=\"black\", size=0.5, linestyle=\"dashed\"\n",
    "    )\n",
    "\n",
    "    def text_fun(dat):\n",
    "        dat = dat.assign(color=\"black\")\n",
    "        return dat\n",
    "\n",
    "    text_data = data_processor(\"text\", text_fun)\n",
    "\n",
    "    def pie_fun(dat):\n",
    "        result = pd.DataFrame()\n",
    "        for _, row in dat.iterrows():\n",
    "            value_df = pd.DataFrame(row[\"value\"], index=[\"end_angle\"]).transpose()\n",
    "            pctgs = value_df[\"end_angle\"] / value_df[\"end_angle\"].sum()\n",
    "            value_df = (value_df / value_df.sum()) * 360\n",
    "            value_df = value_df.cumsum().fillna(0)\n",
    "            value_df[\"start_angle\"] = value_df[\"end_angle\"].shift(1).fillna(0)\n",
    "            value_df = value_df.loc[value_df[\"start_angle\"] != value_df[\"end_angle\"], :]\n",
    "\n",
    "            end_angle = (-1 * value_df[\"start_angle\"] + 90) % 360\n",
    "            start_angle = (-1 * value_df[\"end_angle\"] + 90) % 360\n",
    "            value_df[\"start_angle\"], value_df[\"end_angle\"] = start_angle, end_angle\n",
    "\n",
    "            value_df[\"height\"] = row_height / 2\n",
    "            value_df[\"x0\"] = row[\"x\"]\n",
    "            value_df[\"y0\"] = row[\"y\"]\n",
    "            value_df[\"row_id\"] = row[\"row_id\"]\n",
    "            value_df[\"value\"] = value_df.index\n",
    "            value_df[\"pctgs\"] = pctgs\n",
    "            result = pd.concat([result, value_df])\n",
    "        result = result.dropna(subset=\"value\", axis=0)\n",
    "        dat = result.merge(dat.drop(\"value\", axis=1), on=[\"row_id\"], how=\"left\")\n",
    "        return dat\n",
    "\n",
    "    pie_data = data_processor(\"pie\", pie_fun)\n",
    "\n",
    "    def image_fun(dat):\n",
    "        dat = dat.assign(y0=dat[\"y\"] - row_height, height=row_height, width=row_height)\n",
    "        return dat\n",
    "\n",
    "    image_data = data_processor(\"image\", image_fun)\n",
    "\n",
    "    # Add Annotations\n",
    "    if plot_row_annotation:\n",
    "        row_annotation = row_groups.melt(\n",
    "            id_vars=\"group\", var_name=\"level\", value_name=\"name\"\n",
    "        ).merge(row_pos[[\"group\", \"ymin\", \"ymax\"]], how=\"left\", on=\"group\")\n",
    "\n",
    "        row_annotation = pd.DataFrame(\n",
    "            {\n",
    "                \"ymin\": row_annotation.groupby(\"name\").apply(lambda x: min(x[\"ymin\"])),\n",
    "                \"ymax\": row_annotation.groupby(\"name\").apply(lambda x: max(x[\"ymax\"])),\n",
    "            }\n",
    "        )\n",
    "        row_annotation[\"y\"] = (row_annotation[\"ymin\"] + row_annotation[\"ymax\"]) / 2\n",
    "        row_annotation[\"xmin\"] = -0.5\n",
    "        row_annotation[\"xmax\"] = 5\n",
    "        row_annotation = row_annotation[\n",
    "            (~pd.isna(row_annotation.index)) & (row_annotation.index != \"\")\n",
    "        ]\n",
    "\n",
    "        text_data_rows = pd.DataFrame(\n",
    "            {\n",
    "                \"xmin\": row_annotation[\"xmin\"],\n",
    "                \"xmax\": row_annotation[\"xmax\"],\n",
    "                \"ymin\": row_annotation[\"ymax\"] + row_space,\n",
    "                \"label_value\": [re.sub(\"\\n\", \" \", x) for x in row_annotation.index],\n",
    "                \"ha\": 0,\n",
    "                \"va\": 0.5,\n",
    "                \"fontweight\": \"bold\",\n",
    "                \"ymax\": (row_annotation[\"ymax\"] + row_space) + row_height,\n",
    "            }\n",
    "        )\n",
    "        text_data = pd.concat([text_data, text_data_rows])\n",
    "\n",
    "    if plot_column_annotation:\n",
    "        col_join = column_groups.melt(\n",
    "            id_vars=[\"group\", \"palette\"], var_name=\"level\", value_name=\"name\"\n",
    "        ).merge(column_pos[[\"group\", \"xmin\", \"xmax\"]], how=\"left\", on=\"group\")\n",
    "        text_pct = 0.9\n",
    "        level_heights = pd.DataFrame(\n",
    "            col_join.groupby(\"level\").apply(lambda x: max(x[\"name\"].str.count(\"\\n\"))),\n",
    "            columns=[\"max_newlines\"],\n",
    "        )\n",
    "        level_heights[\"height\"] = (level_heights[\"max_newlines\"] + 1) * text_pct + (\n",
    "            1 - text_pct\n",
    "        )\n",
    "        level_heights[\"levelmatch\"] = pd.Series(\n",
    "            [column_groups.columns.tolist().index(x) for x in level_heights.index],\n",
    "            index=level_heights.index,\n",
    "            name=\"level\",\n",
    "        )\n",
    "        level_heights = level_heights.sort_values([\"levelmatch\"], ascending=False)\n",
    "        level_heights[\"ysep\"] = row_space\n",
    "        level_heights[\"ymax\"] = (\n",
    "            col_annot_offset\n",
    "            + (level_heights[\"height\"] + level_heights[\"ysep\"]).cumsum()\n",
    "            - level_heights[\"ysep\"]\n",
    "        )\n",
    "        level_heights[\"ymin\"] = level_heights[\"ymax\"] - level_heights[\"height\"]\n",
    "        level_heights[\"y\"] = (level_heights[\"ymin\"] + level_heights[\"ymax\"]) / 2\n",
    "        palette_mids = {\n",
    "            x: palettes[x][round(len(palettes[x]) / 2)]\n",
    "            if isinstance(palettes[x], list)\n",
    "            else list(palettes[x].values())[round(len(palettes[x]) / 2)]\n",
    "            for x in palettes.keys()\n",
    "        }\n",
    "        max_newlines = (\n",
    "            col_join.groupby(\"level\")\n",
    "            .apply(lambda x: x[\"name\"].str.count(\"\\n\").max())\n",
    "            .transpose()\n",
    "        )\n",
    "        column_annotation = col_join.merge(\n",
    "            max_newlines.rename(\"max_newlines\"), on=\"level\", how=\"left\"\n",
    "        )\n",
    "        xmin = column_annotation.groupby(\n",
    "            [\"level\", \"name\", \"palette\"], dropna=False\n",
    "        ).apply(lambda x: min(x[\"xmin\"]))\n",
    "        xmax = column_annotation.groupby(\n",
    "            [\"level\", \"name\", \"palette\"], dropna=False\n",
    "        ).apply(lambda x: max(x[\"xmax\"]))\n",
    "        column_annotation = (\n",
    "            pd.concat(\n",
    "                [\n",
    "                    xmin.index.to_frame(),\n",
    "                    xmin.rename(\"xmin\"),\n",
    "                    xmax.rename(\"xmax\"),\n",
    "                    ((xmin + xmax) / 2).rename(\"x\"),\n",
    "                ],\n",
    "                axis=1,\n",
    "            )\n",
    "        ).reset_index(drop=True)\n",
    "\n",
    "        column_annotation = column_annotation.merge(\n",
    "            level_heights, on=\"level\", how=\"left\"\n",
    "        )\n",
    "        column_annotation = column_annotation[~pd.isna(column_annotation[\"name\"])]\n",
    "        column_annotation = column_annotation[\n",
    "            column_annotation[\"name\"].str.contains(\"[a-zA-Z]\")\n",
    "        ]\n",
    "        column_annotation[\"colour\"] = [\n",
    "            palette_mids[col] for col in column_annotation[\"palette\"]\n",
    "        ]\n",
    "        rect_data = pd.concat(\n",
    "            [\n",
    "                rect_data,\n",
    "                pd.DataFrame(\n",
    "                    {\n",
    "                        \"xmin\": column_annotation[\"xmin\"],\n",
    "                        \"xmax\": column_annotation[\"xmax\"],\n",
    "                        \"ymin\": column_annotation[\"ymin\"],\n",
    "                        \"ymax\": column_annotation[\"ymax\"],\n",
    "                        \"colour\": column_annotation[\"colour\"],\n",
    "                        \"alpha\": [\n",
    "                            1 if lm == 0 else 0.25\n",
    "                            for lm in column_annotation[\"levelmatch\"]\n",
    "                        ],\n",
    "                        \"border\": False,\n",
    "                    }\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "        text_data = pd.concat(\n",
    "            [\n",
    "                text_data,\n",
    "                pd.DataFrame(\n",
    "                    {\n",
    "                        \"xmin\": column_annotation[\"xmin\"] + col_space,\n",
    "                        \"xmax\": column_annotation[\"xmax\"] - col_space,\n",
    "                        \"ymin\": column_annotation[\"ymin\"],\n",
    "                        \"ymax\": column_annotation[\"ymax\"],\n",
    "                        \"va\": 0.5,\n",
    "                        \"ha\": 0.5,\n",
    "                        \"fontweight\": [\n",
    "                            \"bold\" if lm == 0 else np.nan\n",
    "                            for lm in column_annotation[\"levelmatch\"]\n",
    "                        ],\n",
    "                        \"colour\": [\n",
    "                            \"white\" if lm == 0 else \"black\"\n",
    "                            for lm in column_annotation[\"levelmatch\"]\n",
    "                        ],\n",
    "                        \"label_value\": column_annotation[\"name\"],\n",
    "                        \"size\": 3. ##FLAG\n",
    "                    }\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        if add_abc:\n",
    "            alphabet = list(map(chr, range(97, 123)))\n",
    "            c_a_df = (\n",
    "                column_annotation[column_annotation[\"levelmatch\"] == 0]\n",
    "                .sort_values(\"x\")\n",
    "                .reset_index(drop=True)\n",
    "            )\n",
    "            text_data_abc = pd.DataFrame(\n",
    "                {\n",
    "                    \"xmin\": c_a_df[\"xmin\"] + col_space,\n",
    "                    \"xmax\": c_a_df[\"xmax\"] - col_space,\n",
    "                    \"ymin\": c_a_df[\"ymin\"],\n",
    "                    \"ymax\": c_a_df[\"ymax\"],\n",
    "                    \"va\": 0.5,\n",
    "                    \"ha\": 0,\n",
    "                    \"fontweight\": \"bold\",\n",
    "                    \"colour\": \"white\",\n",
    "                    \"label_value\": [alphabet[i] + \")\" for i in c_a_df.index],\n",
    "                }\n",
    "            )\n",
    "            text_data = pd.concat([text_data, text_data_abc])\n",
    "\n",
    "    # Add column names\n",
    "    df = column_pos[column_pos[\"name\"] != \"\"]\n",
    "    if df.shape[0] > 0:\n",
    "        df_column_segments = pd.DataFrame(\n",
    "            {\"x\": df[\"x\"], \"xend\": df[\"x\"], \"y\": -0.3, \"yend\": -0.1, \"size\": 4.}\n",
    "        )\n",
    "        segment_data = pd.concat([segment_data, df_column_segments])\n",
    "        df_column_text = pd.DataFrame(\n",
    "            {\n",
    "                \"xmin\": df[\"xmin\"],\n",
    "                \"xmax\": df[\"xmax\"],\n",
    "                \"ymin\": 0,\n",
    "                \"ymax\": col_annot_offset,\n",
    "                \"angle\": col_annot_angle,\n",
    "                \"va\": 0,\n",
    "                \"ha\": 0,\n",
    "                \"label_value\": df[\"name\"],\n",
    "                \"size\": 4.\n",
    "            }\n",
    "        )\n",
    "        text_data = pd.concat([text_data, df_column_text])\n",
    "\n",
    "    # Determine plotting window\n",
    "    minimum_x = min(\n",
    "        [\n",
    "            min(lst, default=np.nan)\n",
    "            for lst in [\n",
    "                column_pos[\"xmin\"],\n",
    "                segment_data[\"x\"],\n",
    "                segment_data[\"xend\"],\n",
    "                rect_data[\"xmin\"],\n",
    "                circle_data[\"x\"] - circle_data[\"r\"],\n",
    "                funkyrect_data[\"x\"] - funkyrect_data[\"r\"],\n",
    "                pie_data[\"xmin\"],\n",
    "                text_data[\"xmin\"],\n",
    "            ]\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    maximum_x = max(\n",
    "        [\n",
    "            max(lst, default=np.nan)\n",
    "            for lst in [\n",
    "                column_pos[\"xmax\"],\n",
    "                segment_data[\"x\"],\n",
    "                segment_data[\"xend\"],\n",
    "                rect_data[\"xmax\"],\n",
    "                circle_data[\"x\"] + circle_data[\"r\"],\n",
    "                funkyrect_data[\"x\"] + funkyrect_data[\"r\"],\n",
    "                pie_data[\"xmax\"],\n",
    "                text_data[\"xmax\"],\n",
    "            ]\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    minimum_y = min(\n",
    "        [\n",
    "            min(lst, default=np.nan)\n",
    "            for lst in [\n",
    "                row_pos[\"ymin\"],\n",
    "                segment_data[\"y\"],\n",
    "                segment_data[\"yend\"],\n",
    "                rect_data[\"ymin\"],\n",
    "                circle_data[\"y\"] - circle_data[\"r\"],\n",
    "                funkyrect_data[\"y\"] - funkyrect_data[\"r\"],\n",
    "                pie_data[\"ymin\"],\n",
    "                text_data[\"ymin\"],\n",
    "            ]\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    maximum_y = max(\n",
    "        [\n",
    "            max(lst, default=np.nan)\n",
    "            for lst in [\n",
    "                row_pos[\"ymax\"],\n",
    "                segment_data[\"y\"],\n",
    "                segment_data[\"yend\"],\n",
    "                rect_data[\"ymax\"],\n",
    "                circle_data[\"y\"] + circle_data[\"r\"],\n",
    "                funkyrect_data[\"y\"] + funkyrect_data[\"r\"],\n",
    "                pie_data[\"ymax\"],\n",
    "                text_data[\"ymax\"],\n",
    "            ]\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    # Simplify certain geoms\n",
    "    if funkyrect_data.shape[0] > 0:\n",
    "        circle_data = pd.concat(\n",
    "            [\n",
    "                circle_data,\n",
    "                funkyrect_data[\n",
    "                    ~np.isnan(funkyrect_data[\"start\"])\n",
    "                    & (funkyrect_data[\"start\"] < 1e-10)\n",
    "                    & (2 * np.pi - 1e-10 < funkyrect_data[\"end\"])\n",
    "                ][[\"x\", \"y\", \"r\", \"colour\"]],\n",
    "            ]\n",
    "        )\n",
    "        funkyrect_data = funkyrect_data[\n",
    "            ~(\n",
    "                ~np.isnan(funkyrect_data[\"start\"])\n",
    "                & (funkyrect_data[\"start\"] < 1e-10)\n",
    "                & (2 * np.pi - 1e-10 < funkyrect_data[\"end\"])\n",
    "            )\n",
    "        ]\n",
    "\n",
    "    rect_data = pd.concat([rect_data, bar_data])\n",
    "    return {\n",
    "        \"row_pos\": row_pos,\n",
    "        \"column_pos\": column_pos,\n",
    "        \"segment_data\": segment_data,\n",
    "        \"rect_data\": rect_data,\n",
    "        \"circle_data\": circle_data,\n",
    "        \"funkyrect_data\": funkyrect_data,\n",
    "        \"pie_data\": pie_data,\n",
    "        \"text_data\": text_data,\n",
    "        \"image_data\": image_data,\n",
    "        \"bounds\": {\n",
    "            \"minimum_x\": minimum_x,\n",
    "            \"maximum_x\": maximum_x,\n",
    "            \"minimum_y\": minimum_y,\n",
    "            \"maximum_y\": maximum_y,\n",
    "        },\n",
    "        \"viz_params\": row_space,\n",
    "    }\n",
    "\n",
    "fh.funkyheatmappy.calculate_positions = calculate_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_funky_heatmap(df, datasets, methods):\n",
    "    plt.figure(dpi=150)\n",
    "\n",
    "    def rescale_to_range(vals, lims):\n",
    "        return vals-np.min(vals)/(np.max(vals)-np.min(vals))*(lims[1]-lims[0])+lims[0]\n",
    "\n",
    "    ## Re-scale values for plotting\n",
    "\n",
    "    mask_denoised = get_denoised_mask(df=df, denoised=denoised)\n",
    "\n",
    "    dd = copy.deepcopy(df)\n",
    "    dd = dd[mask_denoised]\n",
    "    dd = dd[['Dataset', 'Method', 'LocalSP_Mean', 'GlobalSP_Mean', 'BalancedSP_Mean']]\n",
    "    dd = dd.rename(columns={'LocalSP_Mean': 'LocalSP', 'GlobalSP_Mean': 'GlobalSP', 'BalancedSP_Mean': 'BalancedSP'})\n",
    "    dd[['LocalSP', 'GlobalSP', 'BalancedSP']] = MinMaxScaler().fit_transform(dd[['LocalSP', 'GlobalSP', 'BalancedSP']])\n",
    "\n",
    "    ## Gather data for funky heatmap\n",
    "\n",
    "    dhf = pd.DataFrame.from_dict({'id': methods})\n",
    "    dhf.index = methods\n",
    "\n",
    "    col_groups = [pd.NA]\n",
    "    col_names = ['']\n",
    "    col_geoms = ['text']\n",
    "    col_pals = [np.nan]\n",
    "    col_opts = [{'ha': 1., 'width':3., 'size': 4}]\n",
    "    col_legs = [False]\n",
    "\n",
    "    for i, dataset in enumerate(datasets):\n",
    "        mask = dd['Dataset']==dataset\n",
    "        d = dd[mask]\n",
    "        d = d[['LocalSP', 'GlobalSP', 'BalancedSP']]\n",
    "        d = d.rename(columns={'LocalSP': f'LocalSP_{dataset}', 'GlobalSP': f'GlobalSP_{dataset}', 'BalancedSP': f'BalancedSP_{dataset}'})\n",
    "        col_groups.extend([dataset, dataset, dataset])\n",
    "        col_names.extend(['Local SP', 'Global SP', 'Balanced SP'])\n",
    "        col_geoms.extend(['funkyrect', 'funkyrect', 'funkyrect'])\n",
    "        palette_to_use = f'palette{np.mod(i,2)+1}'\n",
    "        col_pals.extend([palette_to_use, palette_to_use, palette_to_use])\n",
    "        col_opts.extend([{}, {}, {}])\n",
    "        col_legs.extend([False, False, False])\n",
    "        d.index = list(methods)\n",
    "        dhf = pd.concat([dhf, d], axis=1)\n",
    "\n",
    "    col_info_df = pd.DataFrame({\n",
    "        'id': list(dhf.columns),\n",
    "        'group': col_groups,\n",
    "        'name': col_names,\n",
    "        'geom': col_geoms,\n",
    "        'options': col_opts,\n",
    "        'palette': col_pals,\n",
    "        'legend': col_legs\n",
    "    })\n",
    "    col_info_df.index = col_info_df['id']\n",
    "\n",
    "    group_pal = np.tile(['palette1', 'palette2'], np.ceil(len(datasets)/2).astype(int))\n",
    "    col_groups_df = pd.DataFrame({\n",
    "        'Category': datasets,\n",
    "        'group': datasets,\n",
    "        'palette': group_pal\n",
    "    })\n",
    "    # col_groups_df.index = datasets\n",
    "    row_info_df = pd.DataFrame({'id': dhf['id'], 'group': ''}, index=dhf['id'])\n",
    "    row_groups_df = pd.DataFrame({'Group': [''], 'group': ['']})\n",
    "\n",
    "    norm = mpl.colors.Normalize(vmin=0, vmax=101, clip=True)\n",
    "    mapper = cm.ScalarMappable(norm=norm, cmap='Greys')\n",
    "    colours_bg = [mapper.to_rgba(i) for i in range(0, 101)]\n",
    "\n",
    "    palettes = pd.DataFrame.from_dict({\n",
    "        'palettes': ['overall', 'palette1', 'palette2'],\n",
    "        'colours': [colours_bg, 'Blues', 'Reds']\n",
    "    })\n",
    "\n",
    "    pos_args = {\n",
    "        'row_height': 1,\n",
    "        'row_space': 0.1,\n",
    "        'row_bigspace': 0.5,\n",
    "        'col_width': 1,\n",
    "        'col_space': 0.1,\n",
    "        'col_bigspace': 0.5,\n",
    "        'col_annot_offset': 3,\n",
    "        'col_annot_angle': 30,\n",
    "        'expand_xmin': 0,\n",
    "        'expand_xmax': 2,\n",
    "        'expand_ymin': 0,\n",
    "        'expand_ymax': 0,\n",
    "        'font_size': 4\n",
    "    }\n",
    "\n",
    "    f = fh.funky_heatmap(\n",
    "        data=dhf,\n",
    "        column_info=col_info_df,\n",
    "        column_groups=col_groups_df,\n",
    "        row_info=row_info_df,\n",
    "        row_groups=row_groups_df,\n",
    "        palettes=palettes,\n",
    "        position_args=pos_args,\n",
    "        add_abc=False,\n",
    "        scale_column=False\n",
    "    )\n",
    "    \n",
    "    plt.title('A', x=.05, y=.88, size=36, weight='bold')\n",
    "\n",
    "    plt.savefig('report/02_funky_heatmap.svg')\n",
    "    plt.savefig('report/02_funky_heatmap.png', dpi=300, transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "plot_funky_heatmap(df_avg, datasets, methods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3.** Plotting $R_{NX}$ curves\n",
    "\n",
    "The $R_{NX}$ curve approximations (from which Local and Global SP are calculated) can be plotted directly for each dataset and method, and we can show the effect of de-noising as well.\n",
    "\n",
    "To show results on denoised inputs, set the argument `denoised` to True, to show results on raw inputs, set it to False.\n",
    "To use denoised inputs only for ViVAE (which it was designed for), set `denoised` to `'ViVAE'`.\n",
    "\n",
    "We display the plot, as well as saving it as a PNG and SVG file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_rnx_curves(rnx, denoised='ViVAE', fname=['./report/03_rnx_curves.svg', './report/03_rnx_curves.png']):\n",
    "\n",
    "    if denoised==True:\n",
    "        this_rnx = copy.deepcopy(rnx['uTrue'])\n",
    "    elif denoised==False:\n",
    "        this_rnx = copy.deepcopy(rnx['uFalse'])\n",
    "    elif denoised=='ViVAE': # use denoised inputs for ViVAE\n",
    "        this_rnx = copy.deepcopy(rnx['uFalse'])\n",
    "        k = list(this_rnx.keys())\n",
    "        for i, key in enumerate(k):\n",
    "            if key.startswith('ViVAE'):\n",
    "                this_rnx[key] = rnx['uTrue'][key]\n",
    "\n",
    "    mpl.rcParams['axes.linewidth'] = 0.2\n",
    "    fig, ax = plt.subplots(nrows=len(datasets), ncols=len(methods), figsize=(.8*len(methods), .7*len(datasets)), sharey=True, sharex=False, dpi=150)\n",
    "    fig.subplots_adjust(hspace=.5)\n",
    "    for i, dataset in enumerate(datasets):\n",
    "        for j, method in enumerate(methods):\n",
    "            curves = this_rnx[dataset][method]\n",
    "            for k, curve in enumerate(curves):\n",
    "                d = curve\n",
    "                ax[i][j].axhline(y=0, xmin=0, xmax=len(curve)+2, lw=.2, ls='--', color='gray')\n",
    "                ax[i][j].plot(range(len(curve)), d, lw=.5)\n",
    "                ax[i][j].xaxis.set_major_formatter(mpl.ticker.FuncFormatter(lambda x, pos: ('{:,.0f}'.format(x/1000) + 'K') if x>0 else '0'))\n",
    "                ax[i][j].tick_params(axis='both', labelsize=5)\n",
    "\n",
    "    pad = 5\n",
    "    for a, col in zip(ax[0], methods):\n",
    "        a.annotate(col, xy=(.4, 1), xytext=(0, pad), xycoords='axes fraction', textcoords='offset points', size=6, weight='bold', ha='left', rotation=45, va='baseline')\n",
    "    for a, row in zip(ax[:,0], datasets):\n",
    "        a.annotate(row, xy=(0, .5), xytext=(0, 0), xycoords=a.yaxis.label, textcoords='offset points', size=6, weight='bold', ha='right', va='center')\n",
    "\n",
    "    fig.suptitle('B', x=-.01, y=.97, size=12, weight='bold')\n",
    "    fig.show()\n",
    "    if fname is not None:\n",
    "        if isinstance(fname, list) and len(fname)>0:\n",
    "            for f in fname:\n",
    "                if f.endswith('.png'):\n",
    "                    fig.savefig(f, bbox_inches='tight', dpi=300)\n",
    "                else:\n",
    "                    fig.savefig(f, bbox_inches='tight')\n",
    "        elif isinstance(fname, str):\n",
    "                fig.savefig(fname, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "plot_rnx_curves(rnx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.** Plotting labelled embeddings\n",
    "\n",
    "We create a plot of embeddings of all datasets by all tested methods, with points coloured by labelled cell populations.\n",
    "Legends for the colour scheme will be saved separately for each dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_embeddings(datasets, methods, denoised='ViVAE', seed=1, palette=palette,\n",
    "        #fname=['./report/03_embeddings.svg', './report/03_embeddings.png']\n",
    "        fname_embedding='./report/04_embeddings.png',\n",
    "        fpath_legends='./report/04_legends'\n",
    "    ):\n",
    "\n",
    "    ## Plot embeddings\n",
    "\n",
    "    fig, ax = plt.subplots(nrows=len(datasets), ncols=len(methods), figsize=(1.4*len(methods), 1.4*len(datasets)), dpi=300)\n",
    "    fig.subplots_adjust(hspace=.35, wspace=.4)\n",
    "    handles = []\n",
    "    labels = []\n",
    "    for i, dataset in enumerate(datasets):\n",
    "        labs = np.load(f'./data/{dataset}_labels.npy', allow_pickle=True)\n",
    "        unas = np.load(f'./data/{dataset}_unassigned.npy', allow_pickle=True).item()\n",
    "        idcs0 = np.array([])\n",
    "        idcs1 = np.arange(len(labs))\n",
    "        s = 5e-2 if len(labs)>30000 else 1e-1\n",
    "        if unas is not None:\n",
    "            idcs0 = np.where(labs == unas)[0]\n",
    "            idcs1 = np.delete(idcs1, idcs0)\n",
    "            labs = np.delete(labs, idcs0)\n",
    "        for j, method in enumerate(methods):\n",
    "            if denoised==False or denoised==True:\n",
    "                emb = np.load(f'./results/{dataset}_{method}_z2_u{denoised}/emb_seed{seed}.npy', allow_pickle=True)\n",
    "            elif denoised=='ViVAE':\n",
    "                if method.startswith('ViVAE'):\n",
    "                    emb = np.load(f'./results/{dataset}_{method}_z2_uTrue/emb_seed1.npy', allow_pickle=True)\n",
    "                else:\n",
    "                    emb = np.load(f'./results/{dataset}_{method}_z2_uFalse/emb_seed1.npy', allow_pickle=True)\n",
    "            \n",
    "            if len(idcs0)>0:\n",
    "                ax[i][j].scatter(emb[idcs0,0], emb[idcs0,1], s=s, c='#bfbfbf', alpha=1., marker='o', linewidths=0)\n",
    "            idx_pop = 0\n",
    "            for pop in np.unique(labs):\n",
    "                idcs = np.where(labs == pop)[0]\n",
    "                ax[i][j].scatter(emb[idcs1[idcs],0], emb[idcs1[idcs],1], label=pop, s=s, c=palette[idx_pop], alpha=1., marker='o', linewidths=0)\n",
    "                ax[i][j].tick_params(axis='both', labelsize=5)\n",
    "                idx_pop += 1\n",
    "            ax[i][j].axis('equal')\n",
    "        this_handles, this_labels = ax[i][0].get_legend_handles_labels()\n",
    "        handles.append(this_handles)\n",
    "        labels.append(this_labels)\n",
    "    pad = 5\n",
    "    for a, col in zip(ax[0], methods):\n",
    "        a.annotate(col, xy=(.4, 1), xytext=(0, pad), xycoords='axes fraction', textcoords='offset points', size=6, weight='bold', ha='center', va='baseline')\n",
    "    for a, row in zip(ax[:,0], datasets):\n",
    "        a.annotate(row, xy=(0, .5), xytext=(0, 0), xycoords=a.yaxis.label, textcoords='offset points', size=6, weight='bold', ha='right', va='center')\n",
    "    fig.suptitle('A', x=.04, y=.89, size=12, weight='bold')\n",
    "    if fname_embedding is not None:\n",
    "        if isinstance(fname_embedding, list) and len(fname_embedding)>0:\n",
    "            for f in fname_embedding:\n",
    "                if f.endswith('.png'):\n",
    "                    fig.savefig(f, bbox_inches='tight', dpi=300)\n",
    "                else:\n",
    "                    fig.savefig(f, bbox_inches='tight')\n",
    "        elif isinstance(fname_embedding, str):\n",
    "                fig.savefig(fname_embedding, bbox_inches='tight')\n",
    "\n",
    "        ## Plot legends\n",
    "\n",
    "        if not os.path.exists(fpath_legends):\n",
    "            os.mkdir(fpath_legends)\n",
    "        for i, dataset in enumerate(datasets):\n",
    "            plt.clf()\n",
    "            plt.axis(False)\n",
    "            l = plt.legend(handles[i], labels[i], loc='center', title=f'{dataset}', title_fontproperties={'weight': 'bold'})\n",
    "            for lh in l.legend_handles: \n",
    "                lh.set_alpha(1.)\n",
    "                lh._sizes = [50]\n",
    "            l._legend_box.align = 'center'\n",
    "            plt.savefig(os.path.join(fpath_legends, f'04_legend_{dataset}.png'), bbox_inches='tight', dpi=300)\n",
    "            plt.savefig(os.path.join(fpath_legends, f'04_legend_{dataset}.svg'), bbox_inches='tight', dpi=300)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "plot_embeddings(datasets, methods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5.** Plotting effects of denoising\n",
    "\n",
    "Our ViVAE pipeline includes nearest neighbour-based denoising ('smoothing') of inputs prior to training the model on them.\n",
    "This typically results in local structures being better preserved in the embedding by ViVAE.\n",
    "Our estimation is that we force ViVAE to model truly important structures and not get overwhelmed by spurious noise patterns.\n",
    "\n",
    "However, in our study we are interested in what effect this denoising might have on other DR methods.\n",
    "In particular, VAE-based methods often benefit from denoising.\n",
    "\n",
    "To document this, we report the effects of denoising for all methods and datasets, as an ablation study.\n",
    "\n",
    "We plot the difference in $R_{NX}$ curves for each method and dataset with and without denoising, and the Local and Global SP shift due to denoising."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_denoising_data(datasets, methods, rnx, sl, sg):\n",
    "    rnx_lims = {}\n",
    "    l_vals0 = {}\n",
    "    l_vals1 = {}\n",
    "    l_diffs = {}\n",
    "    g_vals0 = {}\n",
    "    g_vals1 = {}\n",
    "    g_diffs = {}\n",
    "\n",
    "    l_diff_lims = [0, 0]\n",
    "    g_diff_lims = [0, 0]\n",
    "\n",
    "    for dataset in datasets:\n",
    "\n",
    "        rnx_max = 0.\n",
    "\n",
    "        this_l_vals0 = {}\n",
    "        this_l_vals1 = {}\n",
    "        this_l_diffs = {}\n",
    "\n",
    "        this_g_vals0 = {}\n",
    "        this_g_vals1 = {}\n",
    "        this_g_diffs = {}\n",
    "\n",
    "        for method in methods:\n",
    "\n",
    "            this_rnx_max = np.max(rnx['uFalse'][dataset][method])\n",
    "            if this_rnx_max>rnx_max:\n",
    "                rnx_max = this_rnx_max\n",
    "\n",
    "            this_l_vals0.update({method: np.array([x for x in sl['uFalse'][dataset][method]])})\n",
    "            this_l_vals1.update({method: np.array([x for x in sl['uTrue'][dataset][method]])})\n",
    "            this_l_diffs.update({method: np.mean(this_l_vals1[method])-np.mean(this_l_vals0[method])})\n",
    "\n",
    "            this_g_vals0.update({method: np.array([x for x in sg['uFalse'][dataset][method]])})\n",
    "            this_g_vals1.update({method: np.array([x for x in sg['uTrue'][dataset][method]])})\n",
    "            this_g_diffs.update({method: np.mean(this_g_vals1[method])-np.mean(this_g_vals0[method])})\n",
    "\n",
    "            l_diff_min = np.min(this_l_diffs[method])\n",
    "            if l_diff_min<l_diff_lims[0]:\n",
    "                l_diff_lims[0] = l_diff_min\n",
    "            g_diff_min = np.min(this_g_diffs[method])\n",
    "            if g_diff_min<g_diff_lims[0]:\n",
    "                g_diff_lims[0] = g_diff_min\n",
    "\n",
    "            l_diff_max = np.max(this_l_diffs[method])\n",
    "            if l_diff_max>l_diff_lims[1]:\n",
    "                l_diff_lims[1] = l_diff_max\n",
    "            g_diff_max = np.max(this_g_diffs[method])\n",
    "            if g_diff_max>g_diff_lims[1]:\n",
    "                g_diff_lims[1] = g_diff_max\n",
    "        \n",
    "        rnx_lims.update({dataset: rnx_max})\n",
    "        l_vals0.update({dataset: this_l_vals0})\n",
    "        l_vals1.update({dataset: this_l_vals1})\n",
    "        l_diffs.update({dataset: this_l_diffs})\n",
    "        g_vals0.update({dataset: this_g_vals0})\n",
    "        g_vals1.update({dataset: this_g_vals1})\n",
    "        g_diffs.update({dataset: this_g_diffs})\n",
    "\n",
    "    return rnx_lims, l_vals0, l_vals1, l_diffs, g_vals0, g_vals1, g_diffs, l_diff_lims, g_diff_lims\n",
    "    \n",
    "rnx_lims, l_vals0, l_vals1, l_diffs, g_vals0, g_vals1, g_diffs, l_diff_lims, g_diff_lims = prepare_denoising_data(datasets, methods, rnx, sl, sg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_denoising_sp_change(datasets, methods, l_diffs, g_diffs, l_diff_lims, g_diff_lims):\n",
    "    fig, ax = plt.subplots(ncols=len(methods), nrows=len(datasets), figsize=(len(methods)*.7, len(datasets)), dpi=150)\n",
    "    fig.subplots_adjust(wspace=.4)\n",
    "    mpl.rcParams['axes.linewidth'] = 0.1\n",
    "\n",
    "    for i, dataset in enumerate(datasets):\n",
    "        for j, method in enumerate(methods):\n",
    "            \n",
    "            ax[i][j].set_ylim((-np.max(np.abs(l_diff_lims)), np.max(np.abs(l_diff_lims))))\n",
    "            ax[i][j].set_xlim((.0, .15))\n",
    "            ax[i][j].hlines(y=0., xmin=.0, xmax=.2, color='grey', ls='--', lw=1.)\n",
    "            if i==0 and j==0:\n",
    "                ax[i][j].bar(x=.05, width=.04, height=l_diffs[dataset][method], color='purple', label='Local SP change')\n",
    "            else:\n",
    "                ax[i][j].bar(x=.05, width=.04, height=l_diffs[dataset][method], color='purple')\n",
    "            ax[i][j].set_xticks([])\n",
    "            ax[i][j].tick_params(axis='both', labelsize=5)\n",
    "            ax2 = ax[i][j].twinx()\n",
    "\n",
    "            if j>0:\n",
    "                ax[i][j].set_yticks([])\n",
    "\n",
    "            ax2.set_ylim((-np.max(np.abs(g_diff_lims)), np.max(np.abs(g_diff_lims))))\n",
    "            ax2.hlines(y=0., xmin=.0, xmax=.2, color='grey', ls='--', lw=1.)\n",
    "            if i==0 and j==0:\n",
    "                ax2.bar(x=.1, width=.04, height=g_diffs[dataset][method], color='orange', label='Global SP change')\n",
    "            else:\n",
    "                ax2.bar(x=.1, width=.04, height=g_diffs[dataset][method], color='orange')\n",
    "            ax2.set_xticks([])\n",
    "            ax2.tick_params(axis='both', labelsize=5)\n",
    "            ax2.yaxis.tick_right()\n",
    "\n",
    "            if j<(len(methods)-1):\n",
    "                ax2.set_yticks([])\n",
    "\n",
    "    pad = 5\n",
    "    for a, col in zip(ax[0], methods):\n",
    "        a.annotate(col, xy=(.5, 1), xytext=(0, pad), xycoords='axes fraction', textcoords='offset points', size=6, weight='bold', ha='left', va='baseline', rotation=45)\n",
    "    for a, row in zip(ax[:,0], datasets):\n",
    "        a.annotate(row, xy=(0, .5), xytext=(0, 0), xycoords=a.yaxis.label, textcoords='offset points', size=6, weight='bold', ha='right', va='center')\n",
    "\n",
    "    fig.legend(bbox_to_anchor=(.9, 1.), prop={'size': 5})\n",
    "\n",
    "    fig.suptitle('B', x=-.05, y=.98, size=12, weight='bold')\n",
    "    fig.savefig('./report/05_denoising_sp.png', bbox_inches='tight', dpi=300, transparent=True)\n",
    "    fig.savefig('./report/05_denoising_sp.svg', bbox_inches='tight', transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_denoising_rnx_change(datasets, methods, rnx):\n",
    "    fig, ax = plt.subplots(ncols=len(methods), nrows=len(datasets), sharey=True, sharex=False, figsize=(.8*len(methods), .7*len(datasets)), dpi=150)\n",
    "    fig.subplots_adjust(hspace=.5)\n",
    "    mpl.rcParams['axes.linewidth'] = 0.2\n",
    "\n",
    "    col0 = plt.cm.Blues(np.linspace(.5, 1., nruns))\n",
    "    col1 = plt.cm.Reds(np.linspace(.5, 1., nruns))\n",
    "\n",
    "    for i, dataset in enumerate(datasets):\n",
    "        for j, method in enumerate(methods):\n",
    "            curves0 = np.array(rnx['uFalse'][dataset][method])\n",
    "            avg0 = np.mean(curves0, axis=0)\n",
    "            curves1 = np.array(rnx['uTrue'][dataset][method])\n",
    "            avg1 = np.mean(curves1, axis=0)\n",
    "\n",
    "            for k, curve in enumerate(curves0):\n",
    "                if i==0 and j==0 and k==0:\n",
    "                    ax[i][j].plot(range(len(curve)), curve, lw=.6, color=col0[k], label='Trained on original inputs')\n",
    "                else:\n",
    "                    ax[i][j].plot(range(len(curve)), curve, lw=.6, color=col0[k])\n",
    "            for k, curve in enumerate(curves1):\n",
    "                if i==0 and j==0 and k==0:\n",
    "                    ax[i][j].plot(range(len(curve)), curve, lw=.6, color=col1[k], label='Trained on denoised inputs')\n",
    "                else:\n",
    "                    ax[i][j].plot(range(len(curve)), curve, lw=.6, color=col1[k])\n",
    "            ax[i][j].axhline(y=0, xmin=0, xmax=len(curve)+2, lw=.2, ls='--', color='gray')\n",
    "            ax[i][j].xaxis.set_major_formatter(mpl.ticker.FuncFormatter(lambda x, pos: ('{:,.0f}'.format(x/1000) + 'K') if x>0 else '0'))\n",
    "            ax[i][j].tick_params(axis='both', labelsize=5)\n",
    "            ax[i][j].tick_params(axis='both', labelsize=5)\n",
    "        \n",
    "    pad = 5\n",
    "    for a, col in zip(ax[0], methods):\n",
    "        a.annotate(col, xy=(.4, 1), xytext=(0, pad), xycoords='axes fraction', textcoords='offset points', size=6, weight='bold', ha='left', rotation=45, va='baseline')\n",
    "    for a, row in zip(ax[:,0], datasets):\n",
    "        a.annotate(row, xy=(0, .5), xytext=(0, 0), xycoords=a.yaxis.label, textcoords='offset points', size=6, weight='bold', ha='right', va='center')\n",
    "\n",
    "    l = fig.legend(bbox_to_anchor=(.8, 1.05), prop={'size': 5})\n",
    "    for line in l.get_lines():\n",
    "        line.set_linewidth(2.)\n",
    "\n",
    "    fig.suptitle('A', x=-.01, y=1.01, size=12, weight='bold')\n",
    "    fig.savefig('./report/05_denoising_rnx_curves.png', bbox_inches='tight', dpi=300, transparent=True)\n",
    "    fig.savefig('./report/05_denoising_rnx_curves.svg', bbox_inches='tight', transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "plot_denoising_rnx_change(datasets, methods, rnx)\n",
    "plot_denoising_sp_change(datasets, methods, l_diffs, g_diffs, l_diff_lims, g_diff_lims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **6.** Creating a LaTeX table with structure-preservation results\n",
    "\n",
    "The best way to report structure-preservation results numerically in a paper is to create LaTeX code that generates a table.\n",
    "We do that below.\n",
    "\n",
    "If the `highlight_best` argument is set to `True`, we put the best average score per dataset in each category in bold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_latex_table(df_avg, highlight_best=True):\n",
    "    d = copy.deepcopy(df_avg)\n",
    "\n",
    "    if highlight_best:\n",
    "        idcs_best_localsp = []\n",
    "        idcs_best_globalsp = []\n",
    "        idcs_best_balancedsp = []\n",
    "\n",
    "    for dataset in datasets:\n",
    "        idcs = np.where(d['Dataset']==dataset)[0]\n",
    "        idcs_best_localsp.append(idcs[np.argmax(d['LocalSP_Mean'][idcs])])\n",
    "        idcs_best_globalsp.append(idcs[np.argmax(d['GlobalSP_Mean'][idcs])])\n",
    "        idcs_best_balancedsp.append(idcs[np.argmax(d['BalancedSP_Mean'][idcs])])\n",
    "\n",
    "    d['Denoising'] = ['On' if x==True else 'Off' for x in d['Denoised']]\n",
    "    d['Local SP']  = [f'${np.round(d[\"LocalSP_Mean\"][i], 3)} \\\\pm {np.round(d[\"LocalSP_SD\"][i], 3)}$' for i in range(d.shape[0])]\n",
    "    d['Global SP'] = [f'${np.round(d[\"GlobalSP_Mean\"][i], 3)} \\\\pm {np.round(d[\"GlobalSP_SD\"][i], 3)}$' for i in range(d.shape[0])]\n",
    "    d['Balanced SP'] = [f'${np.round(d[\"BalancedSP_Mean\"][i], 3)} \\\\pm {np.round(d[\"BalancedSP_SD\"][i], 3)}$' for i in range(d.shape[0])]\n",
    "\n",
    "    d = d[['Dataset', 'Method', 'Denoising', 'Local SP', 'Global SP', 'Balanced SP']]\n",
    "\n",
    "    if highlight_best:\n",
    "        for i in idcs_best_localsp:\n",
    "            s = d['Local SP'][i]\n",
    "            d['Local SP'][i] = re.sub('\\$$', '}$', re.sub('^\\$', r'$\\\\mathbf{', s))\n",
    "        for i in idcs_best_globalsp:\n",
    "            s = d['Global SP'][i]\n",
    "            d['Global SP'][i] = re.sub('\\$$', '}$', re.sub('^\\$', r'$\\\\mathbf{', s))\n",
    "        for i in idcs_best_balancedsp:\n",
    "            s = d['Balanced SP'][i]\n",
    "            d['Balanced SP'][i] = re.sub('\\$$', '}$', re.sub('^\\$', r'$\\\\mathbf{', s))\n",
    "\n",
    "    ## Merge adjacent cells containing method names\n",
    "\n",
    "    for method in methods:\n",
    "        idcs = np.where(d['Method']==method)[0]\n",
    "\n",
    "        idcs_multirow = np.array([idcs[i] for i in range(len(idcs)) if np.mod(i, 2)==0])\n",
    "        idcs_empty = np.array([idcs[i] for i in range(len(idcs)) if np.mod(i, 2)==1])\n",
    "\n",
    "        d['Method'][idcs_multirow] = '\\\\multirow{2}{*}{'+method+'}'\n",
    "        d['Method'][idcs_empty] = ''\n",
    "\n",
    "    ## Merge adjacent cells containing dataset names\n",
    "\n",
    "    for dataset in datasets:\n",
    "        idcs = np.where(d['Dataset']==dataset)[0]\n",
    "        n = len(idcs)\n",
    "        d['Dataset'][idcs[0]] = '\\\\multirow{'+str(n)+'}{*}{'+dataset+'}'\n",
    "        for i in range(1, n):\n",
    "            d['Dataset'][idcs[i]] = ''\n",
    "\n",
    "    ## Some adjustments...\n",
    "\n",
    "    d = d.set_index('Dataset', append=True).swaplevel(0, 1)\n",
    "    code = d.to_latex(index=True)\n",
    "    code = re.sub(pattern='} \\& [0-9]+ \\&', repl='} &', string=code)\n",
    "    code = re.sub(pattern='\\\\\\\\\\n \\& [0-9]+ \\&', repl='\\\\\\\\\\n &', string=code)\n",
    "    code = re.sub(pattern='\\&  \\& Method', repl='& Method', string=code)\n",
    "    code = code.replace('$ \\\\\\\\\\n\\\\cline{1-7}\\n\\\\multirow[t]', '$ \\\\\\\\\\n\\\\multirow[t]')\n",
    "    code = code.replace('\\n & Method & Denoising & Local SP & Global SP & Balanced SP \\\\\\\\\\nDataset &', '\\n Dataset & Method & Denoising & Local SP & Global SP & Balanced SP \\\\\\\\\\n &')\n",
    "    code = '{\\n\\\\renewcommand{\\\\arraystretch}{0.65}\\n\\\\tiny'+code+'}'\n",
    "\n",
    "    with open('./report/06_results_table.txt', 'w') as text_file:\n",
    "        text_file.write(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_latex_table(df_avg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ViVAE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
